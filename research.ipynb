{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "research.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Interpause/pseudo-text/blob/master/research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm8XD_Pm4psw",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "If you got here from my repository [Interpause/pseudo-text](https://github.com/Interpause/pseudo-text), you would know these datasets are related to fake news. Fake news classifiers generally fall into three categories: content-based, context-based and style-based. I generally got datasets related to all these, suitable for a range of approaches from claim comparison to credibility calculation to style analysis.<br><br>\n",
        "Tasks:\n",
        "* Get wikipedia dataset despite being general (not newspaper form)\n",
        "* Whats Precision, Recall and F1?\n",
        "* Check up all the numbers for each dataset\n",
        "* Should bot be trained to generate it before classifying it\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GYknk3V5XWY",
        "colab_type": "text"
      },
      "source": [
        "# Fake News Datasets\n",
        "A lot of scraping will be required.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EnnoRoG9KN4",
        "colab_type": "text"
      },
      "source": [
        "## FakeNewsNet\n",
        "https://github.com/KaiDMML/FakeNewsNet<br>https://arxiv.org/abs/1809.01286<br>\n",
        "* source: politifact, gossipcop (problematic? TODO), twitter<br>\n",
        "* samples: uncountable <br>\n",
        "\n",
        "### Minimal\n",
        "* features: url, title, tweet_ids (true/false separated different CSVs)<br>\n",
        "* uses: headlines dataset<br>\n",
        "\n",
        "### Full\n",
        "* features: article, tweets, retweets, user_profile, user_timeline_tweets, followers, following<br>\n",
        "* uses: content dataset, credibility calculation, realistic dataset, everything<br><br>\n",
        "\n",
        "Essentially, a very good scrape of twitter. Issue is scraper has to be manually run over along time. Allows custom approaches where users can be further traced than dataset scraper does. Allows for active scrapping during training? Quite good as its real world data.\n",
        "![Dataset test scores](https://i.imgur.com/wsR0FAC.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEiWaz9BK8yw",
        "colab_type": "text"
      },
      "source": [
        "## Buzzfeed\n",
        "https://github.com/BuzzFeedNews/2018-12-fake-news-top-50<br>\n",
        "* source: buzzfeed, Lead Stories (fact checker)<br>\n",
        "* features: title, url, fb_engagement, date, category, source<br>\n",
        "* features: top 50 fake news articles, lists of fake news sites<br>\n",
        "* usage: content dataset, demo <br>\n",
        "\n",
        "Will need a lot of scrapping. Hey at least those sites probably don't have bot protection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ex7CwKiIodk",
        "colab_type": "text"
      },
      "source": [
        "## Claims Comparison\n",
        "https://www.kaggle.com/c/fake-news/data<br>\n",
        "* source: unknown (TODO)<br>\n",
        "* samples: 26k <br>\n",
        "* features: title, author, text, label<br>\n",
        "\n",
        "https://www.kaggle.com/c/fake-news-pair-classification-challenge/data<br>\n",
        "* source: unknown (TODO)<br>\n",
        "* features: true headline, another headline<br>\n",
        "* labels: same message, refuting, or unrelated<br>\n",
        "* samples: 400k<br>\n",
        "\n",
        "https://github.com/FakeNewsChallenge/fnc-1<br>\n",
        "* source: unknown (TODO)<br>\n",
        "* features: headline, paragraph<br>\n",
        "* labels: agree, disagree, neutral, unrelated<br>\n",
        "* samples: 50k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKm5-EYnHCrF",
        "colab_type": "text"
      },
      "source": [
        "## LIAR\n",
        "https://github.com/thiagorainmaker77/liar_dataset<br>\n",
        "https://arxiv.org/abs/1705.00648<br>\n",
        "* source: politifact (problematic? TODO)<br>\n",
        "* features: statement, topics, speaker, job title, state, party, truth counts, context<br>\n",
        "* usage: claims dataset<br>\n",
        "\n",
        "I have been playing with this dataset. Statements after pre-processing tend to be too short for anything aside from headlines. Dataset as a whole too simple to do much. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1FKmYL85mOd",
        "colab_type": "text"
      },
      "source": [
        "# True News Datasets\n",
        "Hey is that a 1.5Gb dataset? We don't need nothing else (ram explodes)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IrgD7KyMl3U",
        "colab_type": "text"
      },
      "source": [
        "## All the News\n",
        "https://components.one/datasets/all-the-news-articles-dataset/<br>\n",
        "* samples: 204k<br>\n",
        "* features: title, text, publication<br>\n",
        "\n",
        "Its a really large SQLlite database of more news than we will ever need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj1q28jNN22H",
        "colab_type": "text"
      },
      "source": [
        "## Million Headlines\n",
        "https://www.kaggle.com/therohk/million-headlines<br>\n",
        "* samples: it says it in the title<br>\n",
        "\n",
        "It mite be too australian mate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RkF87pOfvF",
        "colab_type": "text"
      },
      "source": [
        "## News Aggregator Dataset\n",
        "https://www.kaggle.com/uciml/news-aggregator-dataset<br>\n",
        "* samples: 423k<br>\n",
        "* features: headlines, URL, category<br>\n",
        "\n",
        "Requires manual scrap, urls may not be intact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeeEHRHOPBTl",
        "colab_type": "text"
      },
      "source": [
        "## News Category Dataset\n",
        "https://www.kaggle.com/rmisra/news-category-dataset<br>\n",
        "* samples: 200k<br>\n",
        "* features: headlines, category<br>\n",
        "\n",
        "Sourced primarily from HuffPost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13dbWedk9oWv",
        "colab_type": "text"
      },
      "source": [
        "# Literature Review\n",
        "Not included out of courtesy, but actual useful reference or seems like a promising read.\n",
        "\n",
        "---\n",
        "\n",
        "To Be Read:\n",
        "* [The Future of Misinformation Detection: New Perspectives and\n",
        "Trends](https://arxiv.org/abs/1909.03654.pdf)\n",
        "* [FakeNewsNet](https://arxiv.org/abs/1809.01286.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DDTXlkX-VPv",
        "colab_type": "text"
      },
      "source": [
        "## News tweets classification with good filtering\n",
        "[Unsupervised Fake News Detection on Social Media:\n",
        "A Generative Approach](http://www.public.asu.edu/~skai2/files/aaai_2019_unsupervised.pdf)<br>\n",
        "Context-based & style-based. Innovative credibility calculation. <br><br>\n",
        "Sentiment analysis only on verified user replies. Credibility calculated by post statistics of said replies. Likes and blank retweets are taken as positive. \"off-the-shelf\" sentiment analysis assuming positive means true, negative means false. 75.9% accuracy on Liar + Buzzfeed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6HnO8nE7pi3",
        "colab_type": "text"
      },
      "source": [
        "# Tools\n",
        "Why is this a section?\n",
        "\n",
        "---\n",
        "\n",
        "* Remember torch has inbuilt transformer layers & LSTMs, and normalizers etc\n",
        "* spaCY has a [text categorizer](https://spacy.io/api/textcategorizer)\n",
        "* Study more about allennlp, you are already using its ELMo embedder\n",
        "* FastAI is even more application-based than spaCY (benchmark?)\n",
        "* Huggingface's transformers and how it supports everything SOTA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaXLfen17u4e",
        "colab_type": "text"
      },
      "source": [
        "# Others\n",
        "**F**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgT5_l9DRKon",
        "colab_type": "text"
      },
      "source": [
        "**Content-based approaches:**\n",
        "* Bag of words (tfidf) and then cluster (done)\n",
        "* Pre-trained ELMo, sum word embeddings & cluster\n",
        "* Claim extraction & comparison\n",
        "    * scrape relevant evidence from internet\n",
        "    * learn facts using large network\n",
        "* Extract raw content, feed gigabytes of content to transformer that has black magic\n",
        "\n",
        "**Style-based approaches:**\n",
        "* Vader social media sentiment analysis\n",
        "* Train transformer on a lot of reviews\n",
        "* Possible to extract information outside of positive or negative? (deadset hard to find)\n",
        "\n",
        "**Context-based approaches:**\n",
        "* Credibility of poster. Credibility of those liking post, their ratings too. I think the UFD one sounds really solid but I am not sure if there is better.\n",
        "* What I proposed but later dropped, actually following interactions between users to determine which users are generally trustable for different topics. Require topic classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz109Ovb7xOW",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Datasets\n",
        "On further examination, this is last priority. Other tools out there are decent.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw0ApNSQ9HwW",
        "colab_type": "text"
      },
      "source": [
        "yknow, a real world fake news identify would probably use context-based to find the suspicious, analyze the suspicious, determine its fakeness, and update a database of credibility. this isnt social credit i swear."
      ]
    }
  ]
}