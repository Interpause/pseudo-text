{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plan.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Interpause/pseudo-text/blob/master/Plan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mXG8iE-a6Ke",
        "colab_type": "text"
      },
      "source": [
        "# A Semi-Supervised approach to gathering relevant credible evidence to use in Automated Fact Checking (AFC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJSBtWDwNX9D",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.imgur.com/dKSd6YN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmgzLnHO-edq",
        "colab_type": "text"
      },
      "source": [
        "# Claims Datasets\n",
        "Note: evidence gathering is completely unsupervised. These datasets come with evidence but they will be used to train the control model only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nguzPOG_lbB",
        "colab_type": "text"
      },
      "source": [
        "## FEVER\n",
        "http://fever.ai<br>\n",
        "Dataset of claim, sentences that are relevant to it, and whether support, refute or not enough info. Goal of challenge is optimize finding evidence sentences from wikipedia then checking the claim using them. Score is only given for correct evidence found (within set of 5). Challenge essentially matches what I am doing except for some [key differences](#fever_differences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NDnZUnFAyTf",
        "colab_type": "text"
      },
      "source": [
        "##SQuAD\n",
        "https://rajpurkar.github.io/SQuAD-explorer<br>\n",
        "The famous question-answer dataset. By modifying it slightly, it can be used to pretrain the classifier to accept the raw article rather than sentences, and it can be modified into a claim checking dataset by converting the questions to statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmHlyu-TBX6-",
        "colab_type": "text"
      },
      "source": [
        "## CREDBANK\n",
        "https://github.com/compsocial/CREDBANK-data<br>\n",
        "While still human-annotated, it is decidedly the noisiest. Unfortunately, the only evidence given is one-liners by the annotator. Which is why this dataset will be used last, relying solely on the approach's ability to gather evidence, to turn this into a regression problem. Might end up reducing performance based on annotation quality though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtHHZ1R4C1H7",
        "colab_type": "text"
      },
      "source": [
        "# Evidence Source Datasets\n",
        "Note: These are pre-gathered. I have no idea yet how to consistently test the online performance of this since scrapers are influenced by many variables. Will be used as the source for the finder for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLrYXXlkDIux",
        "colab_type": "text"
      },
      "source": [
        "## 12K self-gathered dataset\n",
        "See [Google Drive](https://drive.google.com/open?id=14lUCkZWvt50nt19Mxx2C1BKOeMV-XXdX). Shows conceptually at least how the approach might actively scrap news articles. I will try utilize Google's Custom Search API next. Contains URL, category, outlet, text, title and metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IrgD7KyMl3U",
        "colab_type": "text"
      },
      "source": [
        "## All the News\n",
        "https://components.one/datasets/all-the-news-articles-dataset/<br>\n",
        "204K samples, a lot of them are barely useable though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RkF87pOfvF",
        "colab_type": "text"
      },
      "source": [
        "## News Aggregator Dataset\n",
        "https://www.kaggle.com/uciml/news-aggregator-dataset<br>\n",
        "Has 423k samples, except its only the headlines and category. However, if my scrapping efforts has shown anything, I should take that long 423k list of URL as a treasure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6DIS1IxGk6b",
        "colab_type": "text"
      },
      "source": [
        "## Wikipedia\n",
        "https://dumps.wikimedia.org/<br>\n",
        "Has yet to be downloaded. At least its easy to download. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDXCZamQDqqB",
        "colab_type": "text"
      },
      "source": [
        "# Other Datasets\n",
        "Note: these are used in finetuning specific parts of the approach, and hence are less significant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORAuDOa3DzBY",
        "colab_type": "text"
      },
      "source": [
        "## FakeNewsChallenge\n",
        "https://github.com/FakeNewsChallenge/fnc-1<br>\n",
        "Agree, Disagree, Neutral, Unrelated. I am butchering this dataset (Agree, Disagree and Neutral will all be \"related\") to use for training relevance detection regardless of stance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeeEHRHOPBTl",
        "colab_type": "text"
      },
      "source": [
        "## News Category Dataset\n",
        "https://www.kaggle.com/rmisra/news-category-dataset<br>\n",
        "200k samples. Along with the other headlines, category datasets, these are useful for training the finder? well, the raw data is quite large so this might not be needed, unsupervised clustering should be more useful than SVMs on this matter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vqJdQWE8F_U",
        "colab_type": "text"
      },
      "source": [
        "<a id='fever_differences'></a>\n",
        "# Difference between FEVER and my approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZACRhntE7LI",
        "colab_type": "text"
      },
      "source": [
        "- Not just Wikipedia\n",
        "    - Newspapers fundamentally more responsive than encyclopedias\n",
        "    - However, newspapers are less credible, \"noiser\"\n",
        "- Doing regression rather than classification\n",
        "    - Will be done by utilizing CREDBANK's credibility labels\n",
        "    - This AFC is an accessory, not the endpoint\n",
        "    - More forgiving as a scoring criteria\n",
        "- Designing a good scraper is now very important\n",
        "    - Well, I'm just modifying newspaper3k...\n",
        "- My approach is centred around checking; FEVER is heavily based on sentence-extraction\n",
        "    - Experiment 1 utilize **paraphrasing** rather than sentence extraction\n",
        "    - Experiment 2 gives the data raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91--ArUpJlDh",
        "colab_type": "text"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXSZ7Ug9EB4Z",
        "colab_type": "text"
      },
      "source": [
        "## Expressing the problem statement as a speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY0EAujAEHm3",
        "colab_type": "text"
      },
      "source": [
        "### Intro\n",
        "Lets cut to the chase, fake news is a issue. One way people have tried to solve fake news is automated fact checkers.\n",
        "\n",
        "In fact, using a dataset of statement and relevant facts, cutting edge deep learning models have achieved x% accuracy, fairly decent compared to (statistics showing people are fallible).\n",
        "\n",
        "However, this dataset is a huge, roughly half the size of Wikipedia, annotated by humans through an expensive crowdsourcing platform.\n",
        "\n",
        "In reality, news waits for no one, with what is considered true constantly being updated. As such, most automated fact checkers are impractical. But what if there was a way to supply these checkers with the latest facts?\n",
        "\n",
        "Hence,\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB-9kNquJ5gB",
        "colab_type": "text"
      },
      "source": [
        "### Solution Sentence\n",
        "My project utilizes a semi-supervised approach to gather relevant credible facts for determining the factual accuracy of articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "didTIf8EJ7zn",
        "colab_type": "text"
      },
      "source": [
        "### Addon\n",
        "...\n",
        "Aside from fake news detection, such a model could also be used to help teachers mark essay, and likewise assist news outlets in vetting articles.\n",
        "\n",
        "...\n",
        "While the model's performance is not good considering its a new approach, it definitely can help speed up the creation of an even larger human-quality dataset, which can be used to incrementally improve the model or create a better one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr5cpRTUJRL3",
        "colab_type": "text"
      },
      "source": [
        "## FEVER papers\n",
        "- https://arxiv.org/pdf/1809.01479.pdf\n",
        "- https://www.aclweb.org/anthology/D19-6617.pdf\n",
        "- https://www.aclweb.org/anthology/D19-6616.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvxDet7sIomq",
        "colab_type": "text"
      },
      "source": [
        "## Good Overviews\n",
        "- [Fake news as a whole](https://arxiv.org/pdf/1811.00770.pdf)\n",
        "- [Finding similar text](https://medium.com/@adriensieg/text-similarities-da019229c894)\n",
        "- [About AFC](https://ora.ox.ac.uk/objects/uuid:f321ff43-05f0-4430-b978-f5f517b73b9b/download_file?file_format=pdf&safe_filename=graves_factsheet_180226%2BFINAL.pdf&type_of_work=Report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrLkOYlj-WDa",
        "colab_type": "text"
      },
      "source": [
        "## Me thinking very hard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J5lcYSgbNX0",
        "colab_type": "text"
      },
      "source": [
        "Should use SQuAD apparently. Q&A is a short distance from is this true questions. entity is removed, changed to question form, judgable by distance from answer to removed entity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w_9qbIl3849",
        "colab_type": "text"
      },
      "source": [
        "The problem I am trying to solve is fundamentally more similar to SQuAD than it is to FEVER, where approaches use at most 1 or 2 selected sentences (though FEVER 1.0 allowed up to 5), and hence a lot of emphasis is placed on sentence selection. Furthermore, FEVER is designed specifically for checking very simple claims, most requiring only 1 piece of evidence, with only their hardest cases requiring all 5 accepted. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9JPVzQM5HV4",
        "colab_type": "text"
      },
      "source": [
        "Some performance improvement should hopefully be expectable from using a large source (news outlets), though perhaps not within the FEVER dataset, though there may be more noise. Using news outlets as the source immediately makes it more responsive compared to utilizing wikipedia due to the fundamental differencies between a newspaper and encyclopedia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z832GQAz5JHY",
        "colab_type": "text"
      },
      "source": [
        "I will further use CREDBANK to perhaps formulate it as a regression rather than classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHj2qI2068Hv",
        "colab_type": "text"
      },
      "source": [
        "Perhaps, this can be formulated as a challenge against FEVER, which assumes its optimal to train for sentence selection. Experiment 3, which gives the documents whole, might be able to show something in that regard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmVsjj9BI4c5",
        "colab_type": "text"
      },
      "source": [
        "# Paraphraser\n",
        "This is a big black box. I hope I find someone else's code for this. Here's a dataset to use for searching for papers that used it. https://www.aclweb.org/anthology/D16-1034.pdf\n"
      ]
    }
  ]
}