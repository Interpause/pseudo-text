{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Pipe.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nYatCOVHW4uS",
        "TkEuMR1GTyKR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Interpause/pseudo-text/blob/master/The_Pipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPTZq7pUTVxU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "13e6730a-d286-4003-c1bb-d076db44eb80"
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext google.colab.data_table\n",
        "%tensorflow_version 2.x\n",
        "from google.colab import drive #drive.flush_and_unmount()\n",
        "from os import chdir\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "chdir('/content/drive/My Drive/CSIT Internship/code')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAKZQVv4T2JI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pretty much taken from the args parameters defined in train.py\n",
        "class extConfig():\n",
        "    ext_ff_size = 2048\n",
        "    ext_heads = 8\n",
        "    ext_hidden_size = 768\n",
        "    ext_layers = 2\n",
        "    ext_dropout = 0.2\n",
        "    large = False #whether to use 'bert-large-uncased' or 'bert-base-uncased' from pytorch-transformers\n",
        "    temp_dir = '../temp' #useless\n",
        "    finetune_bert = False #whether output of bert model is detached\n",
        "    max_pos = 512\n",
        "    param_init = 'nan' #shouldnt be used since we are loading from checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYatCOVHW4uS",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O79aM1joTcCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    already_run\n",
        "except:\n",
        "    already_run = False\n",
        "if not already_run:\n",
        "    !pip install --upgrade spacy\n",
        "    #!python -m spacy download en_core_web_sm\n",
        "    #!python -m spacy download en_core_web_md\n",
        "    !python -m spacy download en_core_web_lg\n",
        "\n",
        "    !pip install tensorboardX\n",
        "    !pip install pyrouge\n",
        "    !pip install pytorch-transformers\n",
        "    !pip install transformers\n",
        "    !pip install torch==1.1.0\n",
        "\n",
        "    !python -m nltk.downloader wordnet\n",
        "    !python -m nltk.downloader omw\n",
        "\n",
        "    !pip install text2num\n",
        "    \n",
        "already_run = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j72xcu6sUWG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "import functools\n",
        "from text_to_num import alpha2digit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTEvdZl8UCdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chdir('/content/drive/My Drive/CSIT Internship/code/PreSumm/src')\n",
        "import models\n",
        "models.__path__.append('/content/drive/My Drive/CSIT Internship/code/PreSumm/src/models') #workaround for importing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import BertModel\n",
        "\n",
        "from models.encoder import ExtTransformerEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkEuMR1GTyKR",
        "colab_type": "text"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXPgAVzWwvo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#taken from nlpyang/PreSumm/src/models/model_builder.py; minimally modified for a more applicative purpose\n",
        "#actually seems as if model was trained on top of default BERT? cause theres only 1 checkpoint for both large & base\n",
        "\n",
        "#obviously a wrapper. actually useless though.\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, large, temp_dir, finetune=False):\n",
        "        super(Bert, self).__init__()\n",
        "        if(large):\n",
        "            self.model = BertModel.from_pretrained('bert-large-uncased', cache_dir=temp_dir)\n",
        "        else:\n",
        "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
        "\n",
        "        self.finetune = finetune\n",
        "\n",
        "    def forward(self, x, segs, mask):\n",
        "        if(self.finetune):\n",
        "            top_vec, _ = self.model(x, segs, attention_mask=mask)\n",
        "        else:\n",
        "            self.eval()\n",
        "            with torch.no_grad():\n",
        "                top_vec, _ = self.model(x, segs, attention_mask=mask)\n",
        "        return top_vec\n",
        "\n",
        "class ExtSummarizer(nn.Module):\n",
        "    def __init__(self, args, device, checkpoint):\n",
        "        super(ExtSummarizer, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.bert = Bert(args.large, args.temp_dir, args.finetune_bert)\n",
        "\n",
        "        if(args.max_pos>512):\n",
        "            my_pos_embeddings = nn.Embedding(args.max_pos, self.bert.model.config.hidden_size)\n",
        "            my_pos_embeddings.weight.data[:512] = self.bert.model.embeddings.position_embeddings.weight.data\n",
        "            my_pos_embeddings.weight.data[512:] = self.bert.model.embeddings.position_embeddings.weight.data[-1][None,:].repeat(args.max_pos-512,1)\n",
        "            self.bert.model.embeddings.position_embeddings = my_pos_embeddings\n",
        "\n",
        "        self.ext_layer = ExtTransformerEncoder(self.bert.model.config.hidden_size, args.ext_ff_size, args.ext_heads,\n",
        "                                               args.ext_dropout, args.ext_layers)\n",
        "\n",
        "        if checkpoint is not None:\n",
        "            self.load_state_dict(checkpoint['model'], strict=True)\n",
        "        '''\n",
        "        else:\n",
        "            if args.param_init != 0.0:\n",
        "                for p in self.ext_layer.parameters():\n",
        "                    p.data.uniform_(-args.param_init, args.param_init)\n",
        "            if args.param_init_glorot:\n",
        "                for p in self.ext_layer.parameters():\n",
        "                    if p.dim() > 1:\n",
        "                        xavier_uniform_(p)\n",
        "        '''\n",
        "        self.to(device)\n",
        "\n",
        "    '''\n",
        "    Lol some docscript practice time\n",
        "    @param src one-hot tokens (padded)\n",
        "    @param segs alternating type ids of 1 & 0 demarcating sentences (padded)\n",
        "    @param clss indices of [CLS] tokens\n",
        "    @param mask_src attention mask for src (ie 1 for everything, 0 for pads)\n",
        "    @param mask_cls stateful mask of [CLS] tokens for training. Setting as all 1.\n",
        "    '''\n",
        "    #src = idxs, segs = alternating 1 & 0 type ids, clss = indices of [CLS] token\n",
        "    def forward(self, src, segs, clss, mask_src, mask_cls):\n",
        "        top_vec = self.bert(src, segs, mask_src)\n",
        "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
        "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
        "        return sent_scores, mask_cls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa_zEZTITm-h",
        "colab_type": "text"
      },
      "source": [
        "#My pipe components\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIJBjzwBy64n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers', 'encoder', 'ff_actv', 'use_interval', 'rnn_size']\n",
        "class sentence_scorer():\n",
        "\n",
        "    def load_model(self,ckpt_path='../models/bertext_cnndm_transformer.pt',config=extConfig,device='cuda'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.config = config()\n",
        "        self.device = device\n",
        "        checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n",
        "        opt = vars(checkpoint['opt'])\n",
        "        for k in opt.keys():\n",
        "            if k in model_flags:\n",
        "                setattr(self.config, k, opt[k])\n",
        "        self.model = ExtSummarizer(self.config, self.device, checkpoint)\n",
        "\n",
        "    def __call__(self,doc):\n",
        "        src = []\n",
        "        segs = []\n",
        "        clss = []\n",
        "        isZero = True\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            enc = self.tokenizer.encode_plus(sent.text)\n",
        "            if len(src) + len(enc['input_ids']) > self.config.max_pos: continue\n",
        "\n",
        "            clss.append(len(src))\n",
        "            src += enc['input_ids']\n",
        "\n",
        "            if isZero: segs += [0] * len(enc['input_ids'])\n",
        "            else: segs += [1] * len(enc['input_ids'])\n",
        "            isZero = not isZero\n",
        "\n",
        "        mask_cls = [1] * len(clss)\n",
        "\n",
        "        inilen = len(src)\n",
        "        src += [0]*(self.config.max_pos - inilen)\n",
        "        segs += [0]*(self.config.max_pos - inilen)\n",
        "        mask_src = [1]*inilen + [0]*(self.config.max_pos - inilen)\n",
        "        \n",
        "        src = torch.tensor([src],device=self.device)\n",
        "        segs = torch.tensor([segs],device=self.device)\n",
        "        clss = torch.tensor([clss],device=self.device)\n",
        "        mask_src = torch.tensor([mask_src],device=self.device,dtype=torch.uint8)\n",
        "        mask_cls = torch.tensor([mask_cls],device=self.device,dtype=torch.uint8)\n",
        "\n",
        "        result = sorted(zip(self.model(src,segs,clss,mask_src,mask_cls)[0][0].detach().cpu().numpy(),doc.sents),reverse=True)\n",
        "        \n",
        "        doc.set_extension('sentence_scores',default=result,force=True)\n",
        "        return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnCLjDSpTrxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merge_nps = nlp.create_pipe('merge_noun_chunks')\n",
        "class query_extractor():\n",
        "    include_tokens = (\n",
        "        'ADJ',\n",
        "        'ADV',\n",
        "        'NOUN',\n",
        "        'PROPN',\n",
        "        'VERB',\n",
        "        'NUM', #prioritize numbers for cases where ambigious measurement\n",
        "        'X' #assuming unrecognized is important in context of passage scanning\n",
        "    )\n",
        "    #excluding only those included by coarse POS\n",
        "    exclude_tokens = (\n",
        "        'ADD',\n",
        "        'LS',\n",
        "        'NIL',\n",
        "        'WRB',\n",
        "        'WP',\n",
        "        'WP$',\n",
        "        'PRP'\n",
        "    )\n",
        "    #https://nlp.stanford.edu/software/dependencies_manual.pdf\n",
        "    #https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n",
        "    exclude_dependency = ()\n",
        "\n",
        "    @functools.lru_cache(maxsize=256, typed=False)\n",
        "    def convert2noun(self,word):    \n",
        "        synsets = wn.synsets(word)\n",
        "        if len(synsets) == 0: return None\n",
        "        lemmas = [l for s in synsets for l in s.lemmas()]\n",
        "        derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n",
        "        nouns = [l.name() for drf in derivationally_related_forms for l in drf[1] if l.synset().name().split('.')[1] == wn.NOUN]\n",
        "        nouns += [l.name() for l in lemmas if l.synset().name().split('.')[1] == wn.NOUN]\n",
        "        result = [(w, float(nouns.count(w))/len(nouns)) for w in set(nouns)]\n",
        "        result.sort(key=lambda w: -w[1])\n",
        "        if len(result) == 0: return None\n",
        "        #print(result)\n",
        "        return result[0][0]\n",
        "\n",
        "    #exists for conciseness\n",
        "    max_nouns = 3\n",
        "    max_query = 5\n",
        "\n",
        "    #BROKEN UP INTO FUNCTIONS FOR EASY IMPROVEMENT IN FUTURE\n",
        "\n",
        "    def check_throw(self,t):\n",
        "        return not t.is_oov and (t.pos_ not in self.include_tokens or t.tag_ in self.exclude_tokens or t.is_stop)\n",
        "\n",
        "    def assign_keyness(self,sent):\n",
        "        for t in sent: t.set_extension('magnitude',getter=lambda t:np.linalg.norm(t.vector,ord=1),force=True) #np.inf for max, ord=2 for euclidean, ord=1 for manhattan #manhattan most suitable as all 300 dimensions should be perpendicular in nature\n",
        "\n",
        "    def assign_nouns(self,names,nouns):\n",
        "        ans = names[:self.max_nouns]\n",
        "        if len(names) < self.max_nouns:\n",
        "            nouns.sort(key=lambda t: len(t.text.split())/len(t.text)) #longer words but shorter noun phrases likely more important\n",
        "            ans += nouns[:self.max_nouns-len(names)]\n",
        "        return ans\n",
        "\n",
        "    def convert_noun(self,sent):\n",
        "        for t in sent: t.set_extension('noun_form',getter=lambda t:self.convert2noun(t.text),force=True)\n",
        "\n",
        "    def score_token(self,t):\n",
        "        if t.is_oov: return 999\n",
        "        if t.pos_ == 'NUM': return 999\n",
        "        if t.is_currency: return 999\n",
        "        return t._.magnitude\n",
        "\n",
        "    def __call__(self,doc):\n",
        "        queries = []\n",
        "        docc = merge_nps(doc)\n",
        "        for sent,msent in zip(doc.sents,docc.sents):\n",
        "                       \n",
        "            keep = []\n",
        "            names = []\n",
        "            nouns = []\n",
        "\n",
        "            for t in list(sent) + list(msent):\n",
        "                if self.check_throw(t): continue\n",
        "                if t.pos_ == 'PROPN': names.append(t)\n",
        "                elif t.pos_ == 'NOUN': nouns.append(t)\n",
        "                else: keep.append(t)\n",
        "            \n",
        "            #remove sentences with no proper nouns or that only contain them\n",
        "            if len(names) == len(sent) or len(names) == 0: queries.append('')\n",
        "            else:\n",
        "                names = list(set(names))\n",
        "                nouns = list(set(nouns))\n",
        "                keep = list(set(keep))\n",
        "                \n",
        "                query = self.assign_nouns(names,nouns)\n",
        "\n",
        "                #naive sentiment by means of word vector\n",
        "                self.assign_keyness(keep)\n",
        "                keep.sort(key=self.score_token,reverse=True)\n",
        "                query += keep\n",
        "\n",
        "                proc = set()\n",
        "\n",
        "                #Temporarily 'Cancelled\"\n",
        "                #VVV\n",
        "                #TODO: better discrimination of what to convert\n",
        "                self.convert_noun(query[:self.max_query])\n",
        "\n",
        "\n",
        "                for t in query[:self.max_query]:\n",
        "                    if not t.pos_ in ('PROPN','NOUN','NUM','X'):\n",
        "                        if t._.noun_form: proc.add(t._.noun_form)\n",
        "                        else: proc.add(t.text)\n",
        "                        #proc.add(t.text)\n",
        "                    else: proc.add(t.text)\n",
        "                \n",
        "                queries.append(' '.join(list(proc)))\n",
        "                #print(list(zip(keep,[t._.magnitude for t in keep])))\n",
        "                #print(dict((t,(t.dep_,t.head)) for t in query))\n",
        "        doc.set_extension('queries',default=list(zip(doc.sents,queries)),force=True)\n",
        "        return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn_05A0oWxzj",
        "colab_type": "text"
      },
      "source": [
        "#Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6FfOCd8WvIG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f1400682-f179-4c81-b0ab-58dc6270852d"
      },
      "source": [
        "if False and not nlp.has_pipe('merge_noun_chunks'):\n",
        "    merge_nps = nlp.create_pipe('merge_noun_chunks')\n",
        "    nlp.add_pipe(merge_nps,after='parser')\n",
        "\n",
        "if not nlp.has_pipe('merge_entities'):\n",
        "    merge_ents = nlp.create_pipe('merge_entities')\n",
        "    nlp.add_pipe(merge_ents,after='ner')\n",
        "\n",
        "extractor = query_extractor()\n",
        "if nlp.has_pipe('query_extractor'):\n",
        "    nlp.replace_pipe('query_extractor',extractor)\n",
        "else:\n",
        "    nlp.add_pipe(extractor, last=True,name='query_extractor')\n",
        "\n",
        "scorer = sentence_scorer()\n",
        "scorer.load_model()\n",
        "if nlp.has_pipe('sentence_scorer'):\n",
        "    nlp.replace_pipe('sentence_scorer',scorer)\n",
        "else:\n",
        "    nlp.add_pipe(scorer, last=True,name='sentence_scorer')\n",
        "#train textcat to determine if sentence is a claim or similar for checking?\n",
        "import gc\n",
        "gc.collect()\n",
        "nlp.pipeline"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f78a2f6fe10>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f78a2dd0348>),\n",
              " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f78a2dd03a8>),\n",
              " ('merge_entities', <function spacy.pipeline.functions.merge_entities>),\n",
              " ('query_extractor', <__main__.query_extractor at 0x7f77f4afb320>),\n",
              " ('sentence_scorer', <__main__.sentence_scorer at 0x7f77f4afb6d8>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Efs-3LnXf21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "59938fd0-c759-4d93-8d55-b6bb6d1ec2d0"
      },
      "source": [
        "#very liberal essay courtesy of me after a culture studies course\n",
        "test1 = '''It is undeniable that Singapore has enjoyed great peace in regards to matters of race, especially compared to other nations such as the United States which currently remain polarised on the matter. Nonetheless, I think that rather than these issues not existing, they were suppressed very well, festering unnoticed. In this essay I will be reflecting on an example of discrimination in Singapore that I know of, and why and why not they may be acceptable. \n",
        "\tWhile idealists like me dream of a day where such a word is meaningless except in the annals of History, for now, and many years to come, race still holds great significance. Why does physical appearance, our skin colour, matter so much? The answer, it does not. Rather, it happened to correlate to other features, features collectively known as Ethnicity, due to Geographical and Historical factors. And it continues to propagate its significance through stereotyping and active polarising influences. \n",
        "\tLet us start with one of the most prominent things that correlates with race, tradition, a subset of the Culture associated with different races. Tradition is one of the common attributes that can draw people together. While the education system has been trying to get people of different races of understand each others Culture through initiatives such as Racial Harmony Day, it cannot be denied that on a normal basis one would better understand a custom their family (mostly homogenous in Singapore, will explain later) forces them to undergo rather than that of another race. This however creates an unseen privilege in regards to this. Objectively, there are quite a few disruptive Chinese traditions, such as the Hungry Ghost Festival (which can be described as “artificial haze”) or Lion Dances (very loud). I am not sure about other races, in itself an illustration of the above point, but I have heard that other racial Cultures similarly have loud traditions. Hence it can be generalized that such historical and hence racially separated tradition tends to be noisy. \n",
        "\tWhat happens now? Well, Singapore is about three quarters Chinese. A Chinese is best able to appreciate their own historical Culture compared to those of other races. What this means is such traditional rites being carried out publically appear perfectly fine to the majority (by which I mean Chinese) in Singapore. However, what about those of other races? Due to the lack of appreciation for their culture, and the rareness of it (it is not accepted as a fact of life), their traditional events may be considered disruptive or even silly by the majority. And because there is a lack of understanding in the first place, close friends are not able to dispel the misconceptions, and may even become influenced. This is can be a polarising force. \n",
        "\tEven worse, Singapore’s initiatives at conserving our heritages have turned into initiatives to conserve Chinese heritage. Take for example the scale of the Speak Mandarin movement. To most Chinese, it would not even come to mind that this is out of the ordinary. Ask one and they probably would not know about initiatives for any other races’ Culture. All of this is very insidious. It may not appear to have effects, but it creates a hotbed for small mistakes and misconceptions that can spiral into disasters. Naturally, there will be individuals who believe in strongly in their stereotypes, but in an age where communication is so easy, such stereotypes can spread more easily than ever before, no longer restrained by temporal limits, let alone restrained by awareness.\n",
        "\tHowever, the situation in Singapore has yet to become a full blown disaster, due to the emphasis placed on other dividing characteristics such as those of Meritocracy. However, look no further than the United States to find a worst case scenario. Already sabotaged historically, the racial polarisation in United States only grows worse due to multiplicity of polarised cultural groups, rampant stereotypes of those groups and general atmosphere of chaos and anger, encouraged by a free media that values sensationalism. Physical appearance there is both so correlated and associated with ethnicity due to the polarisation resulting in tribalistic instincts emerging, and often that container of ethnicity is not light, and depending on your “alignment” carries many strong stereotypes both good and bad. However, for Singapore to get from here to that, it would be near impossible. On its own, Singaporean society values ability and money too much to get caught up with it, and are on average too well educated to easily accept stereotypes. So what would it take?\n",
        "\tGetting to my policy suggestion, I believe Singapore should cease its emphasis on our cultural heritage. No more Speak Mandarin Movement as one example. That is a very bold statement to make, with equally bold reasons why. Firstly, Culture is not something that is governed, it is something that governs itself. Maybe some cultural analysts think what is happening in Singapore is cultural dilution, but I think not. Let cultural fusion happen, even if it borders on cultural appropriation, because what it allows for is a real shared heritage everyone can appreciate. Even when this seemingly results in cultural homogeneity, it can still be vibrant in relation to the rest of the world, and is much better than having another racial riot. Race is a very divisive thing due to all its historical baggage and resulting constructs, such dilution can serve to weaken the concept of race, creating new Culture along lines that are not race. And that is what youth today naturally gravitate towards. In the ages of seeking one’s own identity, but also in this age of information saturation and cultural dilution which allows for greater understanding of it, youth no longer define their identity along race as heavily anymore. However, initiatives such as the Speak Mandarin Movement go counter to this. They are designed to force youths to define their identity along their ethnicity, and consequently their race. By placing too heavy an emphasis on the Historical baggage, it will only result in majority of people mainly seeing each other by their race again, and even worse, push the ever changing field of culture to remain divided along race, and possibly even push it into the sort of vicious cycles of racial identity as seen in other nations. \n",
        "\tSecondly, we should encourage youths to create their own culture, from a very young age. Our education system could have children in primary school play games related to the nature of culture. They could be encouraged to come up with their own tribes with made up customs, once in a while swapping the groups of course, and have pseudo-conflicts to better ingrain the nature of culture into youths so they do not make the same mistake of placing emphasis on things (race for example) that have been overemphasized for too long.\n",
        "\tOf course, with the current direction Singapore is going, the conservative nature of the majority here, all of this will be hard to execute. There are many people who genuinely believe it is right, for no reason, to continue our cultural heritage by forcing it to remain exactly the same and passed down. I do not believe in such things as there being a right culture, and think that practically, the best solution to prevent racial polarisation is simply cultural fusion. Naturally, youths are heading in that direction, but there are also many being pushed to remain conservative on culture, and mainly strongly refrain from even thinking about a topic regarding as Out of Bounds. It is impossible and wrong to get people to simply abandon their existing culture and force them to adapt to change just because it would seem more efficient. It is the preference of many not to change after all, and so massive change should not be the solution. \n",
        "\tMoving forwards I do not think Singapore will ever descend into complete chaos due to race before we are eliminated by something else, like for example a meteor strike (aka it will not happen). Our historical and geographical situations and limitations means we will not fall down that rabbit hole. However, it is possible Singapore will become a bit more aware of the racial differences, which in itself is a bad thing if not followed up by appreciation of other ethnic cultures. It is already happening after all.\n",
        "\tRegarding the course, I found it more of a mentally stimulating activity. They were things I learnt, such as better expression in regards to these areas and the proper terminologies. I also had misconceptions around terms such as gender being dispelled. I have also become more sensitive to the existences of the various femininities and masculinities, and hence can exploit code switching better. Race-wise, I have learnt more about how the situation in the United States came to be. However, I have always believe in the day that physical appearance will no longer matter as part of one’s identity, and personally strive towards the conditions to create such a world. In that regards, being more aware of racial and sexual divides has allowed me to understand better the conditions that lead to them and gives me room for thought on how to mitigate them.\n",
        "'''\n",
        "with nlp.disable_pipes('sentence_scorer'):\n",
        "    doc1 = nlp(alpha2digit(test1,'en'))\n",
        "[s[1] for s in doc1._.queries if s[1] != '']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Singapore the United States polarization matters compare',\n",
              " 'Singapore muser discrimination acceptability an example',\n",
              " 'correlation start different races tradition the Culture',\n",
              " 'Singapore Culture undergo homogeny Racial Harmony Day',\n",
              " 'the Hungry Ghost Festival artificial haze quite a few disruptive Chinese traditions loudness Lion Dances',\n",
              " 'Singapore Chinese about 3 quarters',\n",
              " 'appreciator A Chinese other races compare their own historical Culture',\n",
              " 'Singapore Chinese fineness such traditional rites publically',\n",
              " 'mind most Chinese coming ordinary',\n",
              " 'Singapore Meritocracy other dividing characteristics place',\n",
              " 'the United States look a worst case scenario discovery',\n",
              " 'sensationalism growth value multiplicity United States',\n",
              " 'Singapore impossibleness approach',\n",
              " 'Singapore getting end our cultural heritage my policy suggestion',\n",
              " 'No more Speak Mandarin Movement one example',\n",
              " 'Singapore some cultural analysts thought materialisation cultural dilution',\n",
              " 'weakener resulting constructs all its historical baggage server new Culture',\n",
              " 'initiatives counter the Speak Mandarin Movement',\n",
              " 'Singapore hardness the current direction the conservative nature execution',\n",
              " 'Conservative culture that direction abstainer Bounds',\n",
              " 'Singapore descent complete chaos example elimination',\n",
              " 'Singapore awareness observation the racial differences appreciation',\n",
              " 'the United States the situation Race impudence memory']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MZpMxLXYLz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "8ad3bcab-2d1e-498c-e2c7-00c8a2e42f13"
      },
      "source": [
        "#https://www.straitstimes.com/world/united-states/chinese-restaurants-in-the-us-are-closing-and-thats-a-good-thing-the-owners-say\n",
        "test2 = ''' More than 40 years after buying Eng's, a Chinese-American restaurant in the Hudson Valley, Tom Sit is reluctantly considering retirement.  For much of his life, Mr Sit has worked here seven days a week, 12 hours a day. He cooks in the same kitchen where he worked as a young immigrant from China. He parks in the same space where he'd take breaks and read his wife's letters, sent from Montreal while they courted by post in the late 1970s. He seats his regulars at the same tables where his three daughters did homework.  Two years ago, at the insistence of his wife, Mrs Faye Lee Sit, he started taking off one day a week. Still, it's not sustainable. He's 76, and they're going to be grandparents soon. Working 80 hours a week is just too hard.  But his grown daughters, who have college degrees and good-paying jobs, don't intend to take over.  Across the country, owners of Chinese-American restaurants like Eng's are ready to retire but have no one to pass the business to. Their children, educated and raised in America, are pursuing professional careers that do not demand the same gruelling labor as food service.  According to new data from the restaurant reviewing website Yelp, the share of Chinese restaurants in the top 20 metropolitan areas has been consistently falling. Five years ago, an average of 7.3 per cent of all restaurants in these areas were Chinese, compared with 6.5 per cent today. That reflects 1,200 fewer Chinese restaurants at a time when these 20 places added more than 15,000 restaurants overall.  Even in San Francisco, home to the oldest Chinatown in the United States, the share of Chinese restaurants shrank to 8.8 per cent from 10 per cent.  It doesn't seem that interest in the cuisine has faltered. On Yelp, the average share of page views of Chinese restaurants hasn't declined, nor has the average rating.  And at the same time, the percentages of Indian, Korean and Vietnamese restaurants - many of which were also owned and operated by immigrants from Asian countries - are holding steady or increasing nationwide.  The restaurant business has always been tough, and rising rents and delivery apps haven't helped. Tightening regulations on immigration and accounting have also made it harder for cash-based restaurants to do business.  But those are not factors specific to Chinese restaurants, and do not explain the wave of closings. Instead, a big reason seems to be the economic mobility of the second generation.  \"It's a success that these restaurants are closing,\" said Ms Jennifer Lee, a former New York Times journalist who wrote of the rise of Chinese restaurants in her book The Fortune Cookie Chronicles and produced a documentary The Search For General Tso.  \"These people came to cook so their children wouldn't have to, and now their children don't have to.\"  The retirements of the restaurant owners also reflect the history of Chinese immigration to the United States. In 1882, the Chinese Exclusion Act halted what had been a steady rise in people coming from China. It was not revoked until 1943, and large-scale immigration resumed only after 1965, when other race-targeting quotas were abolished.  China's Cultural Revolution, an often-violent social and political upheaval that started in 1966, prompted many young people to emigrate to the US, a country that projected an image of freedom and economic possibility.  Mr Sit left Guangzhou, in southern China, in 1968. He hiked, climbed and swam his way to Hong Kong, filling his pants with pine cones as an improvised flotation device.  \"There was just no future,\" he said. \"The only way to get freedom and to get a good job was to go to Hong Kong.\"  In 1974, he immigrated to the US and started working at Eng's, which opened in 1927. He had never worked in a restaurant before, but the heat from the woks was much less intense than what he experienced at a Hong Kong plastics factory where he had worked.  Unlike Mr Sit, some immigrants had been chefs in China. They served Hunan and Cantonese foods on linen tablecloths to bejewelled, curious diners at places like Shun Lee Palace in New York.  \"There was the golden age of Chinese cooking in America, starting in the late 1960s and early 1970s,\" said Mr Ed Schoenfeld, a restaurateur and chef who had worked in Chinese restaurants since the 1970s. \"We started getting regional practitioners of fine regional cuisine to come to this country and do their thing.\"  Mostly, though, the newly minted chefs cooked quickly and cheaply. They adapted their method of cooking to American tastes, developing dishes like beef chow fun, fortune cookies and egg drop soup, often brought home in the signature takeout containers.  \"They were not precious,\" Ms Lee said. \"These people did not come to be chefs; they came to be immigrants, and cooking was the way they made a living.\"  Other immigrant groups follow a similar pattern. With social mobility and inclusion in more mainstream parts of the economy, the children of immigrants are less likely than their parents to own their own businesses.  \"In some ways, the children are regaining the status of the first generation that they have lost while migrating,\" said sociology professor Jennifer Lee of Columbia University and co-author of The Asian American Achievement Paradox. (She is not related to Ms Lee, the journalist.) \"The goal has never been to continue those businesses.\"  When they do become entrepreneurs, these children tend to work in industries like tech or consulting, rather than in food service or nail salons.  In the past decade, some members of the second generation have also chosen to take charge of family restaurants. Nom Wah Tea Parlor, a New York dim sum restaurant that opened in 1920, has stayed a family business: first run by the Choy family, then the Tangs.  The 41-year-old owner, Mr Wilson Tang, left a career in finance to succeed his uncle in 2011. Initially, his parents baulked at his decision.  \"As immigrants, it's the only thing you can do; if it's not restaurants, it's a laundromat,\" he said. \"For me to choose to go back to owning a restaurant? That was tough for them to accept.\"  Since then, Nom Wah has expanded - to another Manhattan location, to Philadelphia and to Shenzhen, China. On any given night, groups of guests wait for a table outside the Chinatown location for up to an hour, huddled in the bend of Doyers Street.  \"I had this unique opportunity to preserve something that was from old New York,\" Mr Tang said. \"I still work extremely hard. But I also know how to use marketing tools, like the Internet.\"  In a parallel effort, the team behind Junzi Kitchen, a fast-casual Chinese restaurant chain based in New York, recently raised US$5 million (S$6.7 million) to research and buy places like Eng's, rebranding them with Junzi's modern take on the cuisine.  \"They are still going to have their usual beloved Chinese takeout services, but we are providing an upgraded version of that,\" said Mr Yong Zhao, the founder and chief executive.  But family-run Chinese restaurants are typically not being passed to the next generation. Some may close up shop, sell their businesses to other first-generation immigrants or move on and see their former storefronts become something else entirely.  Mr Sit has not yet found the right person to run the restaurant, and has no immediate plans to close.  \"To take over Eng's, you have to keep the heart in Eng's,\" he said. \"You need to have a loyalty to the business, not just someone who thinks, 'I'll make one year, two years of money, I don't care.'\"  Mrs Sit feels more ready to retire than her husband. Normally talkative, he can be evasive whenever the family tries to bring up a successor.  \"They'll have to work hard,\" she said, her eyes sparkling as she teased her husband. \"Like Tom Sit. Maybe then he'll let them take over.\"  If he ever actually does hand Eng's to someone else, Mr Sit will miss his customers, and miss running an operation.  But he is proud of what he built. He is proud that his daughters, American-born educated professionals, are working in jobs they have chosen, jobs they love.  \"I hoped they have a better life than me,\" he said. \"A good life. And they do.\"'''\n",
        "with nlp.disable_pipes('query_extractor'):\n",
        "    doc2 = nlp(alpha2digit(test2,'en'))\n",
        "doc2._.sentence_scores"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.44611275,\n",
              "  More than 40 years after buying Eng's, a Chinese-American restaurant in the Hudson Valley, Tom Sit is reluctantly considering retirement.  ),\n",
              " (0.31025773,\n",
              "  Across the country, owners of Chinese-American restaurants like Eng's are ready to retire but have no one to pass the business to.),\n",
              " (0.15926246,\n",
              "  According to new data from the restaurant reviewing website Yelp, the share of Chinese restaurants in the top 20 metropolitan areas has been consistently falling.),\n",
              " (0.14281066,\n",
              "  5 years ago, an average of 7.3 per cent of all restaurants in these areas were Chinese, compared with 6.5 per cent today.),\n",
              " (0.0868622,\n",
              "  On Yelp, the average share of page views of Chinese restaurants hasn't declined, nor has the average rating.  ),\n",
              " (0.08647902,\n",
              "  Even in San Francisco, home to the oldest Chinatown in the United States, the share of Chinese restaurants shrank to 8.8 per cent from 10 per cent.  ),\n",
              " (0.08198352,\n",
              "  But his grown daughters, who have college degrees and good-paying jobs, don't intend to take over.  ),\n",
              " (0.068967886,\n",
              "  The restaurant business has always been tough, and rising rents and delivery apps haven't helped.),\n",
              " (0.0642307,\n",
              "  That reflects 1,200 fewer Chinese restaurants at a time when these 20 places added more than 15,000 restaurants overall.  ),\n",
              " (0.06270016, Working 80 hours a week is just too hard.  ),\n",
              " (0.060862273,\n",
              "  Their children, educated and raised in America, are pursuing professional careers that do not demand the same gruelling labor as food service.  ),\n",
              " (0.053114243,\n",
              "  And at the same time, the percentages of Indian, Korean and Vietnamese restaurants - many of which were also owned and operated by immigrants from Asian countries - are holding steady or increasing nationwide.  ),\n",
              " (0.041162223,\n",
              "  But those are not factors specific to Chinese restaurants, and do not explain the wave of closings.),\n",
              " (0.03868964,\n",
              "  For much of his life, Mr Sit has worked here 7 days a week, 12 hours a day.),\n",
              " (0.038275786,\n",
              "  He cooks in the same kitchen where he worked as a young immigrant from China.),\n",
              " (0.033010118,\n",
              "  Instead, a big reason seems to be the economic mobility of the second generation.  ),\n",
              " (0.031834774,\n",
              "  2 years ago, at the insistence of his wife, Mrs Faye Lee Sit, he started taking off one day a week.),\n",
              " (0.031128308, It doesn't seem that interest in the cuisine has faltered.),\n",
              " (0.02884388, He's 76, and they're going to be grandparents soon.),\n",
              " (0.028475812, Still, it's not sustainable.),\n",
              " (0.02500972,\n",
              "  Tightening regulations on immigration and accounting have also made it harder for cash-based restaurants to do business.  ),\n",
              " (0.01359219,\n",
              "  He seats his regulars at the same tables where his 3 daughters did homework.  ),\n",
              " (0.011194023,\n",
              "  He parks in the same space where he'd take breaks and read his wife's letters, sent from Montreal while they courted by post in the late 1970s.)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bci4enYhYd8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01f905b0-8615-4dde-fd63-f1cada11cac6"
      },
      "source": [
        "#https://demo.allennlp.org/open-information-extraction\n",
        "#!pip install allennlp\n",
        "#OEI seems promising, but doesn't seem to reliably generate coherent sentences.\n",
        "#^Preferring coherence over simplicity"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 4.9MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.3)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 73.0MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/86/7a/532fc167366797f66c732549490dcf13243077f15446115f3c0ad17e56b8/overrides-2.6.tar.gz\n",
            "Collecting spacy<2.2,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.9MB 100kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting torch>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6MB 24kB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 60.0MB/s \n",
            "\u001b[?25hCollecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: h5py in /tensorflow-2.1.0/python3.6 (from allennlp) (2.10.0)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 55.9MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.40)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from allennlp) (1.17.4)\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Requirement already satisfied: requests>=2.18 in /tensorflow-2.1.0/python3.6 (from allennlp) (2.22.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from tensorboardX>=1.2->allennlp) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /tensorflow-2.1.0/python3.6 (from tensorboardX>=1.2->allennlp) (3.11.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /tensorflow-2.1.0/python3.6 (from flask>=1.0.2->allennlp) (0.16.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.10.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.7MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.4.2)\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.9)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.85)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.40)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests>=2.18->allennlp) (1.25.7)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests>=2.18->allennlp) (2019.11.28)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.1.0/python3.6 (from pytest->allennlp) (42.0.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->allennlp) (0.15.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Building wheels for collected packages: ftfy, word2number, overrides, jsonnet, parsimonious, numpydoc\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=09322f072e27794686174ff7308d7d5d74b4bb708e3ecbd438c1b9518c5f982d\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=e591a722a06562636566a54df4cb2cd4140a5d4b685653919ec5c69fcb2e2f39\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.6-cp36-none-any.whl size=5523 sha256=5ce0e32f90926100e727824659b4cb9a3829c3ca1053bd489c8bef8b36523757\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/86/49/c413319bcff638bdc13462c063c84d68e294d84514170c3744\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320333 sha256=cbbea4384509eb58ea938da6d1335ee459483a9b599fdd22bb2ccb814b723555\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=676f57bf369dd5a0a18bbcc21861bc9a0a2b85d3c6bd65f79eea22bf133df9bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31894 sha256=06af31bde2a9d1e06cb7b8b3970efee61ba9b34dfcc238c38610261e352352be\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
            "Successfully built ftfy word2number overrides jsonnet parsimonious numpydoc\n",
            "\u001b[31mERROR: en-core-web-lg 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: ftfy, word2number, jsonpickle, flask-cors, unidecode, overrides, blis, preshed, thinc, spacy, torch, jsonnet, parsimonious, pytorch-transformers, responses, flaky, conllu, numpydoc, pytorch-pretrained-bert, allennlp\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: thinc 7.3.1\n",
            "    Uninstalling thinc-7.3.1:\n",
            "      Successfully uninstalled thinc-7.3.1\n",
            "  Found existing installation: spacy 2.2.3\n",
            "    Uninstalling spacy-2.2.3:\n",
            "      Successfully uninstalled spacy-2.2.3\n",
            "  Found existing installation: torch 1.1.0\n",
            "    Uninstalling torch-1.1.0:\n",
            "      Successfully uninstalled torch-1.1.0\n",
            "  Found existing installation: pytorch-transformers 1.2.0\n",
            "    Uninstalling pytorch-transformers-1.2.0:\n",
            "      Successfully uninstalled pytorch-transformers-1.2.0\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.2 overrides-2.6 parsimonious-0.8.1 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.9 spacy-2.1.9 thinc-7.0.8 torch-1.3.1 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "blis",
                  "preshed",
                  "pytorch_transformers",
                  "spacy",
                  "thinc",
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ygB47mdLhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}