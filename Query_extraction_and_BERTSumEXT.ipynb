{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Query extraction and BERTSumEXT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CAun264BvP9U",
        "2_NIL9m49oRR",
        "K5W0B8e_92Hr"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Interpause/pseudo-text/blob/master/Query_extraction_and_BERTSumEXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSHdkD33mNMV",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/sebastianruder/NLP-progress/blob/master/english/temporal_processing.md\n",
        "\n",
        "Interesting, but I don't think I can properly harness this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuhFTT4lqaVM",
        "colab_type": "code",
        "outputId": "fede862a-a126-4920-a165-c6dc96658db0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext google.colab.data_table\n",
        "%tensorflow_version 2.x\n",
        "from google.colab import drive #drive.flush_and_unmount()\n",
        "from os import chdir\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "chdir('/content/drive/My Drive/CSIT Internship/code')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAun264BvP9U",
        "colab_type": "text"
      },
      "source": [
        "#Wordnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZVe9cVdmLic",
        "colab_type": "code",
        "outputId": "c5a41339-2d69-4972-e26e-93343828b86a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mc-aCggqLjX",
        "colab_type": "code",
        "outputId": "dbf6d154-0433-4645-feaf-b1dec1b0fcd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "wn.synset('dog.n.01').hyponyms()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('basenji.n.01'),\n",
              " Synset('corgi.n.01'),\n",
              " Synset('cur.n.01'),\n",
              " Synset('dalmatian.n.02'),\n",
              " Synset('great_pyrenees.n.01'),\n",
              " Synset('griffon.n.02'),\n",
              " Synset('hunting_dog.n.01'),\n",
              " Synset('lapdog.n.01'),\n",
              " Synset('leonberg.n.01'),\n",
              " Synset('mexican_hairless.n.01'),\n",
              " Synset('newfoundland.n.01'),\n",
              " Synset('pooch.n.01'),\n",
              " Synset('poodle.n.01'),\n",
              " Synset('pug.n.01'),\n",
              " Synset('puppy.n.01'),\n",
              " Synset('spitz.n.01'),\n",
              " Synset('toy_dog.n.01'),\n",
              " Synset('working_dog.n.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beOunh4IqRU2",
        "colab_type": "code",
        "outputId": "68a3647b-4290-4e0f-81b7-00f45826543b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for s in wn.synsets('polarised'):\n",
        "    print(s,s.hypernyms())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synset('polarize.v.01') [Synset('change.v.01')]\n",
            "Synset('polarize.v.02') [Synset('separate.v.02')]\n",
            "Synset('polarize.v.03') [Synset('separate.v.12')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tsdh7k2r4Ps",
        "colab_type": "code",
        "outputId": "51af8eb0-0091-4a2b-c421-4181f51935c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_all_hypernyms': None,\n",
              " '_wordnet_corpus_reader': <WordNetCorpusReader in '/root/nltk_data/corpora/wordnet'>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDttiXHM9VgK",
        "colab_type": "code",
        "outputId": "0a5597ef-c2f7-4f2d-d659-4847a7cb9e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wn.lemma('stupid.a.01.stupid').antonyms()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('smart.a.01.smart')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQXAcUtm9y23",
        "colab_type": "code",
        "outputId": "0ca454b2-5429-4391-faa4-5424a556f156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(wn.synset('intelligent.a.01').path_similarity(wn.synset('smart.a.01')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aHvWeWlN_Qi",
        "colab_type": "code",
        "outputId": "1e69b936-2fd8-459e-b126-78bda0573a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from nltk.corpus import wordnet_ic\n",
        "nltk.download('wordnet_ic')\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "a = wn.synset('stupid.a.01')\n",
        "b = wn.synset('ability.n.01')\n",
        "wn.res_similarity(a, b, brown_ic)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5962292078977726"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUX5oJrSPOCA",
        "colab_type": "code",
        "outputId": "f289288a-827f-44af-843a-de049f69eb3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a.hypernyms()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('ability.n.02')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFZBZ_ZsTz8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        " \n",
        "# Just to make it a bit more readable\n",
        "WN_NOUN = 'n'\n",
        "WN_VERB = 'v'\n",
        "WN_ADJECTIVE = 'a'\n",
        "WN_ADJECTIVE_SATELLITE = 's'\n",
        "WN_ADVERB = 'r'\n",
        " \n",
        "def convert(word, from_pos, to_pos):    \n",
        "    \"\"\" Transform words given from/to POS tags \"\"\"\n",
        " \n",
        "    synsets = wn.synsets(word, pos=from_pos)\n",
        " \n",
        "    # Word not found\n",
        "    if not synsets:\n",
        "        return []\n",
        " \n",
        "    # Get all lemmas of the word (consider 'a'and 's' equivalent)\n",
        "    lemmas = [l for s in synsets\n",
        "                for l in s.lemmas()\n",
        "                if s.name().split('.')[1] == from_pos\n",
        "                    or from_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE)\n",
        "                        and s.name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE)]\n",
        " \n",
        "    # Get related forms\n",
        "    derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n",
        " \n",
        "    # filter only the desired pos (consider 'a' and 's' equivalent)\n",
        "    related_noun_lemmas = [l for drf in derivationally_related_forms\n",
        "                             for l in drf[1] \n",
        "                             if l.synset().name().split('.')[1] == to_pos\n",
        "                                or to_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE)\n",
        "                                    and l.synset().name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE)]\n",
        " \n",
        "    # Extract the words from the lemmas\n",
        "    words = [l.name() for l in related_noun_lemmas]\n",
        "    len_words = len(words)\n",
        " \n",
        "    # Build the result in the form of a list containing tuples (word, probability)\n",
        "    result = [(w, float(words.count(w))/len_words) for w in set(words)]\n",
        "    result.sort(key=lambda w: -w[1])\n",
        " \n",
        "    # return all the possibilities sorted by probability\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_VCZgmyU6gr",
        "colab_type": "code",
        "outputId": "99b3585e-3fab-4a06-e9c0-e9a41ace02f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "convert(\"shit\", WN_ADJECTIVE, WN_NOUN)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlokDfLqVBOT",
        "colab_type": "code",
        "outputId": "3671d783-318f-484a-a9f5-a15fdb58751b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = wn.synset('person.n.01')\n",
        "a.attributes()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4MvGKTwXh4Q",
        "colab_type": "code",
        "outputId": "47dc8dd7-0d3c-4edb-e99d-be92fad5af35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wn.synsets('person')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('person.n.01'), Synset('person.n.02'), Synset('person.n.03')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLbgk-btX0XW",
        "colab_type": "code",
        "outputId": "3c2c6d26-e569-4f8a-e7d5-9babdf98417f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = wn.synset('stupid.a.01')\n",
        "a.part_holonyms() + a.member_holonyms() + a.substance_holonyms()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8s8rYb-adqp",
        "colab_type": "code",
        "outputId": "edde31c6-dc11-4e08-a92e-32abb5e2fbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a.tree(lambda s:s.hypernyms())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('stupid.a.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPiITr_ldLpE",
        "colab_type": "code",
        "outputId": "a7b8fdb1-83ab-402b-b606-db53b47d8ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ".definition()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a girl or young woman with whom a man is romantically involved'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqTOSLsqfzNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = wn.synset('girlfriend.n.02')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTRhoXdXxlql",
        "colab_type": "code",
        "outputId": "97ddca6c-9302-47e6-854c-2acc54469cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "for s in wn.synsets('dog'):\n",
        "    print(s.definition())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
            "a dull unattractive unpleasant girl or woman\n",
            "informal term for a man\n",
            "someone who is morally reprehensible\n",
            "a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll\n",
            "a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward\n",
            "metal supports for logs in a fireplace\n",
            "go after with the intent to catch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFHWwpk-gKqQ",
        "colab_type": "code",
        "outputId": "22831bfb-4c50-4992-e59c-5784c61b5648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wn.synset('dog.n.01').attributes()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH6QhoG0gQyA",
        "colab_type": "code",
        "outputId": "1cbbfbc5-9093-4154-ac7f-6d73e5158e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(wn.morphy('doggie', wn.NOUN))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doggie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEZWN8LBvUnz",
        "colab_type": "text"
      },
      "source": [
        "#Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KecN8SStg0I-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install --upgrade spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!python -m spacy download en_core_web_md\n",
        "#!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "\n",
        "#!python -m nltk.downloader wordnet\n",
        "#!python -m nltk.downloader omw\n",
        "#!pip install spacy-wordnet\n",
        "#from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#from textblob import TextBlob\n",
        "\n",
        "import functools\n",
        "#!pip install clone https://github.com/huggingface/neuralcoref.git\n",
        "\n",
        "#!pip install text2num\n",
        "from text_to_num import alpha2digit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZ6D4SQ9FrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "displacy_options = {\n",
        "    'compact':True,\n",
        "    'color':'#FF00FF',\n",
        "    'bg':'#00FF00',\n",
        "    'font':'Comic Sans MS',\n",
        "    #'fine_grained':True,\n",
        "    'collapse_phrases':True,\n",
        "    'offset_x':50,\n",
        "    'word_spacing':20,\n",
        "    'distance':80\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWCciqVwm92i",
        "colab_type": "text"
      },
      "source": [
        "TODO:\n",
        "- Create advanced logic for extracting keywords\n",
        "    - NER: find entity dependencies...\n",
        "    - Filter out tokens based on their dependency role\n",
        "    - Include otherwise skipped tokens if they govern certain dependencies\n",
        "- Figure out what is universally removed nonetheless (embed into coarse + fine exclusions)\n",
        "- Rank entities/dependencies to include/global TFIDF\n",
        "- Create scoring for tokens to future support ensembling\n",
        "- Split pipe into several pipe steps\n",
        "    - Add score & preferred position property\n",
        "    - Add global TFIDF to score\n",
        "    - Add Sentiment (polarity/subjectivity) to score\n",
        "    - Add advanced dependency analysis to score\n",
        "    - Add word pos/tag filter to score\n",
        "    - All this is soft weighing... Consider hard removal\n",
        "- Use rule matcher to pre-remove personal opinion sentences...\n",
        "- split sentences on commas and 'because/as/cause' too.\n",
        "- Come up with rules as to when to use convert2noun function (accurate but stupid)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmwSKTtuil8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merge_nps = nlp.create_pipe('merge_noun_chunks')\n",
        "class query_extractor():\n",
        "    include_tokens = (\n",
        "        'ADJ',\n",
        "        'ADV',\n",
        "        'NOUN',\n",
        "        'PROPN',\n",
        "        'VERB',\n",
        "        'NUM', #prioritize numbers for cases where ambigious measurement\n",
        "        'X' #assuming unrecognized is important in context of passage scanning\n",
        "    )\n",
        "    #excluding only those included by coarse POS\n",
        "    exclude_tokens = (\n",
        "        'ADD',\n",
        "        'LS',\n",
        "        'NIL',\n",
        "        'WRB',\n",
        "        'WP',\n",
        "        'WP$',\n",
        "        'PRP'\n",
        "    )\n",
        "    #https://nlp.stanford.edu/software/dependencies_manual.pdf\n",
        "    #https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n",
        "    exclude_dependency = ()\n",
        "\n",
        "    @functools.lru_cache(maxsize=256, typed=False)\n",
        "    def convert2noun(self,word):    \n",
        "        synsets = wn.synsets(word)\n",
        "        if len(synsets) == 0: return None\n",
        "        lemmas = [l for s in synsets for l in s.lemmas()]\n",
        "        derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n",
        "        nouns = [l.name() for drf in derivationally_related_forms for l in drf[1] if l.synset().name().split('.')[1] == wn.NOUN]\n",
        "        nouns += [l.name() for l in lemmas if l.synset().name().split('.')[1] == wn.NOUN]\n",
        "        result = [(w, float(nouns.count(w))/len(nouns)) for w in set(nouns)]\n",
        "        result.sort(key=lambda w: -w[1])\n",
        "        if len(result) == 0: return None\n",
        "        #print(result)\n",
        "        return result[0][0]\n",
        "\n",
        "    #exists for conciseness\n",
        "    max_nouns = 3\n",
        "    max_query = 5\n",
        "\n",
        "    #BROKEN UP INTO FUNCTIONS FOR EASY IMPROVEMENT IN FUTURE\n",
        "\n",
        "    def check_throw(self,t):\n",
        "        return not t.is_oov and (t.pos_ not in self.include_tokens or t.tag_ in self.exclude_tokens or t.is_stop)\n",
        "\n",
        "    def assign_keyness(self,sent):\n",
        "        for t in sent: t.set_extension('magnitude',getter=lambda t:np.linalg.norm(t.vector,ord=1),force=True) #np.inf for max, ord=2 for euclidean, ord=1 for manhattan #manhattan most suitable as all 300 dimensions should be perpendicular in nature\n",
        "\n",
        "    def assign_nouns(self,names,nouns):\n",
        "        ans = names[:self.max_nouns]\n",
        "        if len(names) < self.max_nouns:\n",
        "            nouns.sort(key=lambda t: len(t.text.split())/len(t.text)) #longer words but shorter noun phrases likely more important\n",
        "            ans += nouns[:self.max_nouns-len(names)]\n",
        "        return ans\n",
        "\n",
        "    def convert_noun(self,sent):\n",
        "        for t in sent: t.set_extension('noun_form',getter=lambda t:self.convert2noun(t.text),force=True)\n",
        "\n",
        "    def score_token(self,t):\n",
        "        if t.is_oov: return 999\n",
        "        if t.pos_ == 'NUM': return 999\n",
        "        if t.is_currency: return 999\n",
        "        return t._.magnitude\n",
        "\n",
        "    def __call__(self,doc):\n",
        "        queries = []\n",
        "        docc = merge_nps(doc)\n",
        "        for sent,msent in zip(doc.sents,docc.sents):\n",
        "                       \n",
        "            keep = []\n",
        "            names = []\n",
        "            nouns = []\n",
        "\n",
        "            for t in list(sent) + list(msent):\n",
        "                if self.check_throw(t): continue\n",
        "                if t.pos_ == 'PROPN': names.append(t)\n",
        "                elif t.pos_ == 'NOUN': nouns.append(t)\n",
        "                else: keep.append(t)\n",
        "            \n",
        "            #remove sentences with no proper nouns or that only contain them\n",
        "            if len(names) == len(sent) or len(names) == 0: queries.append('')\n",
        "            else:\n",
        "                names = list(set(names))\n",
        "                nouns = list(set(nouns))\n",
        "                keep = list(set(keep))\n",
        "                \n",
        "                query = self.assign_nouns(names,nouns)\n",
        "\n",
        "                #naive sentiment by means of word vector\n",
        "                self.assign_keyness(keep)\n",
        "                keep.sort(key=self.score_token,reverse=True)\n",
        "                query += keep\n",
        "\n",
        "                proc = set()\n",
        "\n",
        "                #Temporarily 'Cancelled\"\n",
        "                #VVV\n",
        "                #TODO: better discrimination of what to convert\n",
        "                #self.convert_noun(query[:self.max_query])\n",
        "\n",
        "\n",
        "                for t in query[:self.max_query]:\n",
        "                    if not t.pos_ in ('PROPN','NOUN','NUM','X'):\n",
        "                        #if t._.noun_form: proc.add(t._.noun_form)\n",
        "                        #else: proc.add(t.text)\n",
        "                        proc.add(t.text)\n",
        "                    else: proc.add(t.text)\n",
        "                \n",
        "                queries.append(' '.join(list(proc)))\n",
        "                #print(list(zip(keep,[t._.magnitude for t in keep])))\n",
        "                #print(dict((t,(t.dep_,t.head)) for t in query))\n",
        "        doc.set_extension('queries',default=list(zip(doc.sents,queries)),force=True)\n",
        "        return doc\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Z9TfR5C3GZ",
        "colab_type": "code",
        "outputId": "fd2295d1-6f84-4238-f592-b85f002dc825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "if False and not nlp.has_pipe('merge_noun_chunks'):\n",
        "    merge_nps = nlp.create_pipe('merge_noun_chunks')\n",
        "    nlp.add_pipe(merge_nps,after='parser')\n",
        "\n",
        "if not nlp.has_pipe('merge_entities'):\n",
        "    merge_ents = nlp.create_pipe('merge_entities')\n",
        "    nlp.add_pipe(merge_ents,after='ner')\n",
        "\n",
        "#seems adjectives that should be converted to nouns are usually alone\n",
        "#hence, handy trick to place it after merger instead of tagger (merger actually breaks? wordnet tags)\n",
        "if False and not nlp.has_pipe('WordnetAnnotator'):\n",
        "    wordnetter = WordnetAnnotator(nlp.lang)\n",
        "    nlp.add_pipe(wordnetter, after='tagger')\n",
        "\n",
        "extractor = query_extractor()\n",
        "if nlp.has_pipe('query_extractor'):\n",
        "    nlp.replace_pipe('query_extractor',extractor)\n",
        "else:\n",
        "    nlp.add_pipe(extractor, last=True,name='query_extractor')\n",
        "#train textcat to determine if sentence is a claim or similar for checking?\n",
        "nlp.pipeline"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f5d473ceb00>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f5d47246a68>),\n",
              " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f5d47246ac8>),\n",
              " ('merge_entities', <function spacy.pipeline.functions.merge_entities>),\n",
              " ('query_extractor', <__main__.query_extractor at 0x7f5d1ff66128>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZdRrd7m7vJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = '''It is undeniable that Singapore has enjoyed great peace in regards to matters of race, especially compared to other nations such as the United States which currently remain polarised on the matter. Nonetheless, I think that rather than these issues not existing, they were suppressed very well, festering unnoticed. In this essay I will be reflecting on an example of discrimination in Singapore that I know of, and why and why not they may be acceptable. \n",
        "\tWhile idealists like me dream of a day where such a word is meaningless except in the annals of History, for now, and many years to come, race still holds great significance. Why does physical appearance, our skin colour, matter so much? The answer, it does not. Rather, it happened to correlate to other features, features collectively known as Ethnicity, due to Geographical and Historical factors. And it continues to propagate its significance through stereotyping and active polarising influences. \n",
        "\tLet us start with one of the most prominent things that correlates with race, tradition, a subset of the Culture associated with different races. Tradition is one of the common attributes that can draw people together. While the education system has been trying to get people of different races of understand each others Culture through initiatives such as Racial Harmony Day, it cannot be denied that on a normal basis one would better understand a custom their family (mostly homogenous in Singapore, will explain later) forces them to undergo rather than that of another race. This however creates an unseen privilege in regards to this. Objectively, there are quite a few disruptive Chinese traditions, such as the Hungry Ghost Festival (which can be described as “artificial haze”) or Lion Dances (very loud). I am not sure about other races, in itself an illustration of the above point, but I have heard that other racial Cultures similarly have loud traditions. Hence it can be generalized that such historical and hence racially separated tradition tends to be noisy. \n",
        "\tWhat happens now? Well, Singapore is about three quarters Chinese. A Chinese is best able to appreciate their own historical Culture compared to those of other races. What this means is such traditional rites being carried out publically appear perfectly fine to the majority (by which I mean Chinese) in Singapore. However, what about those of other races? Due to the lack of appreciation for their culture, and the rareness of it (it is not accepted as a fact of life), their traditional events may be considered disruptive or even silly by the majority. And because there is a lack of understanding in the first place, close friends are not able to dispel the misconceptions, and may even become influenced. This is can be a polarising force. \n",
        "\tEven worse, Singapore’s initiatives at conserving our heritages have turned into initiatives to conserve Chinese heritage. Take for example the scale of the Speak Mandarin movement. To most Chinese, it would not even come to mind that this is out of the ordinary. Ask one and they probably would not know about initiatives for any other races’ Culture. All of this is very insidious. It may not appear to have effects, but it creates a hotbed for small mistakes and misconceptions that can spiral into disasters. Naturally, there will be individuals who believe in strongly in their stereotypes, but in an age where communication is so easy, such stereotypes can spread more easily than ever before, no longer restrained by temporal limits, let alone restrained by awareness.\n",
        "\tHowever, the situation in Singapore has yet to become a full blown disaster, due to the emphasis placed on other dividing characteristics such as those of Meritocracy. However, look no further than the United States to find a worst case scenario. Already sabotaged historically, the racial polarisation in United States only grows worse due to multiplicity of polarised cultural groups, rampant stereotypes of those groups and general atmosphere of chaos and anger, encouraged by a free media that values sensationalism. Physical appearance there is both so correlated and associated with ethnicity due to the polarisation resulting in tribalistic instincts emerging, and often that container of ethnicity is not light, and depending on your “alignment” carries many strong stereotypes both good and bad. However, for Singapore to get from here to that, it would be near impossible. On its own, Singaporean society values ability and money too much to get caught up with it, and are on average too well educated to easily accept stereotypes. So what would it take?\n",
        "\tGetting to my policy suggestion, I believe Singapore should cease its emphasis on our cultural heritage. No more Speak Mandarin Movement as one example. That is a very bold statement to make, with equally bold reasons why. Firstly, Culture is not something that is governed, it is something that governs itself. Maybe some cultural analysts think what is happening in Singapore is cultural dilution, but I think not. Let cultural fusion happen, even if it borders on cultural appropriation, because what it allows for is a real shared heritage everyone can appreciate. Even when this seemingly results in cultural homogeneity, it can still be vibrant in relation to the rest of the world, and is much better than having another racial riot. Race is a very divisive thing due to all its historical baggage and resulting constructs, such dilution can serve to weaken the concept of race, creating new Culture along lines that are not race. And that is what youth today naturally gravitate towards. In the ages of seeking one’s own identity, but also in this age of information saturation and cultural dilution which allows for greater understanding of it, youth no longer define their identity along race as heavily anymore. However, initiatives such as the Speak Mandarin Movement go counter to this. They are designed to force youths to define their identity along their ethnicity, and consequently their race. By placing too heavy an emphasis on the Historical baggage, it will only result in majority of people mainly seeing each other by their race again, and even worse, push the ever changing field of culture to remain divided along race, and possibly even push it into the sort of vicious cycles of racial identity as seen in other nations. \n",
        "\tSecondly, we should encourage youths to create their own culture, from a very young age. Our education system could have children in primary school play games related to the nature of culture. They could be encouraged to come up with their own tribes with made up customs, once in a while swapping the groups of course, and have pseudo-conflicts to better ingrain the nature of culture into youths so they do not make the same mistake of placing emphasis on things (race for example) that have been overemphasized for too long.\n",
        "\tOf course, with the current direction Singapore is going, the conservative nature of the majority here, all of this will be hard to execute. There are many people who genuinely believe it is right, for no reason, to continue our cultural heritage by forcing it to remain exactly the same and passed down. I do not believe in such things as there being a right culture, and think that practically, the best solution to prevent racial polarisation is simply cultural fusion. Naturally, youths are heading in that direction, but there are also many being pushed to remain conservative on culture, and mainly strongly refrain from even thinking about a topic regarding as Out of Bounds. It is impossible and wrong to get people to simply abandon their existing culture and force them to adapt to change just because it would seem more efficient. It is the preference of many not to change after all, and so massive change should not be the solution. \n",
        "\tMoving forwards I do not think Singapore will ever descend into complete chaos due to race before we are eliminated by something else, like for example a meteor strike (aka it will not happen). Our historical and geographical situations and limitations means we will not fall down that rabbit hole. However, it is possible Singapore will become a bit more aware of the racial differences, which in itself is a bad thing if not followed up by appreciation of other ethnic cultures. It is already happening after all.\n",
        "\tRegarding the course, I found it more of a mentally stimulating activity. They were things I learnt, such as better expression in regards to these areas and the proper terminologies. I also had misconceptions around terms such as gender being dispelled. I have also become more sensitive to the existences of the various femininities and masculinities, and hence can exploit code switching better. Race-wise, I have learnt more about how the situation in the United States came to be. However, I have always believe in the day that physical appearance will no longer matter as part of one’s identity, and personally strive towards the conditions to create such a world. In that regards, being more aware of racial and sexual divides has allowed me to understand better the conditions that lead to them and gives me room for thought on how to mitigate them.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6sYSvT9v-hk",
        "colab_type": "code",
        "outputId": "2ee6ef2e-9965-456e-835f-623bc0217403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "doc = nlp(alpha2digit(test,'en'))\n",
        "displacy.render(list(doc.sents)[17], style=\"dep\",jupyter=True,options=displacy_options)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"26f9a1f3482a489497c8457f9c0144c6-0\" class=\"displacy\" width=\"1010\" height=\"142.0\" direction=\"ltr\" style=\"max-width: none; height: 142.0px; color: #FF00FF; background: #00FF00; font-family: Comic Sans MS; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">A Chinese</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">best</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">able</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">appreciate</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">their own historical Culture</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">compared</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">those</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">other races.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-0\" stroke-width=\"2px\" d=\"M62,82.0 62,68.66666666666667 127.0,68.66666666666667 127.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M62,84.0 L58,76.0 66,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-1\" stroke-width=\"2px\" d=\"M142,82.0 142,68.66666666666667 207.0,68.66666666666667 207.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M207.0,84.0 L211.0,76.0 203.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-2\" stroke-width=\"2px\" d=\"M142,82.0 142,55.33333333333333 290.0,55.33333333333333 290.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M290.0,84.0 L294.0,76.0 286.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-3\" stroke-width=\"2px\" d=\"M382,82.0 382,68.66666666666667 447.0,68.66666666666667 447.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M382,84.0 L378,76.0 386,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-4\" stroke-width=\"2px\" d=\"M302,82.0 302,55.33333333333333 450.0,55.33333333333333 450.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M450.0,84.0 L454.0,76.0 446.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-5\" stroke-width=\"2px\" d=\"M462,82.0 462,68.66666666666667 527.0,68.66666666666667 527.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M527.0,84.0 L531.0,76.0 523.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-6\" stroke-width=\"2px\" d=\"M462,82.0 462,55.33333333333333 610.0,55.33333333333333 610.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M610.0,84.0 L614.0,76.0 606.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-7\" stroke-width=\"2px\" d=\"M622,82.0 622,68.66666666666667 687.0,68.66666666666667 687.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M687.0,84.0 L691.0,76.0 683.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-8\" stroke-width=\"2px\" d=\"M702,82.0 702,68.66666666666667 767.0,68.66666666666667 767.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M767.0,84.0 L771.0,76.0 763.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-9\" stroke-width=\"2px\" d=\"M782,82.0 782,68.66666666666667 847.0,68.66666666666667 847.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M847.0,84.0 L851.0,76.0 843.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-26f9a1f3482a489497c8457f9c0144c6-0-10\" stroke-width=\"2px\" d=\"M862,82.0 862,68.66666666666667 927.0,68.66666666666667 927.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-26f9a1f3482a489497c8457f9c0144c6-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M927.0,84.0 L931.0,76.0 923.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUxP0WdxnonU",
        "colab_type": "code",
        "outputId": "a50c1845-29de-407b-ac78-dbebcd3b5d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "[s[1] for s in doc._.queries if s[1] != '']"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Singapore polarised compared regards the United States',\n",
              " 'Singapore this essay discrimination reflecting acceptable',\n",
              " 'different races the Culture start correlates tradition',\n",
              " 'Singapore Racial Harmony Day homogenous undergo Culture',\n",
              " 'loud artificial haze quite a few disruptive Chinese traditions the Hungry Ghost Festival Lion Dances',\n",
              " 'about 3 quarters Singapore Chinese',\n",
              " 'other races compared A Chinese appreciate their own historical Culture',\n",
              " 'Singapore publically Chinese such traditional rites fine',\n",
              " 'come ordinary most Chinese mind',\n",
              " 'other dividing characteristics Singapore placed Meritocracy',\n",
              " 'a worst case scenario the United States look find',\n",
              " 'values multiplicity grows sensationalism United States',\n",
              " 'impossible Singapore near',\n",
              " 'our cultural heritage Singapore Getting my policy suggestion cease',\n",
              " 'Mandarin Movement No more Speak one example',\n",
              " 'Singapore happening think some cultural analysts cultural dilution',\n",
              " 'new Culture such dilution serve resulting constructs weaken',\n",
              " 'initiatives the Speak Mandarin Movement counter',\n",
              " 'the current direction Singapore hard the conservative nature execute',\n",
              " 'that direction Bounds conservative refrain culture',\n",
              " 'Singapore example complete chaos eliminated descend',\n",
              " 'aware Singapore the racial differences followed appreciation',\n",
              " 'the situation wise the United States learnt Race']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qs-nv1fGBw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token = doc[25]\n",
        "print(token)\n",
        "#https://spacy.io/api/annotation#named-entities\n",
        "print(token.ent_type_)\n",
        "\n",
        "#https://spacy.io/api/annotation#pos-universal\n",
        "print(token.pos_) #Spacy's general\n",
        "print(token.tag_) #OntoNotes damn specific ones\n",
        "\n",
        "#https://spacy.io/api/annotation#dependency-parsing\n",
        "print(token.dep_)\n",
        "print(token.head)\n",
        "\n",
        "#how confident model was in assigning tags (how to use?)\n",
        "print(token.prob)\n",
        "\n",
        "print(token.sentiment)\n",
        "\n",
        "print(token._.wordnet.synsets())\n",
        "print(token._.wordnet.lemmas())\n",
        "\n",
        "# Can be used to better inform adj to noun process (pick specific category: requires classification..)\n",
        "print(token._.wordnet.wordnet_domains())\n",
        "\n",
        "#textblob.sentiment to prioritize sentences (highlight stronger? scan first? scan harder? scan limit?)\n",
        "#textblob.sentiment_assessments (per token) to pick words to use in query (create spacy pipe for it)\n",
        "#use wordnet to map kept adj/adv to nouns if possible!!!\n",
        "#re-finetune abstractive summarization to do queries?\n",
        "#^find dataset of text2query\n",
        "#^or train on mix of my generations + abstraction tasks (preserve coherence)\n",
        "\n",
        "\n",
        "#token vectors, sentimentality, similarity... etc only offered on larger models\n",
        "#also includes brown clusters\n",
        "#use brown clustering? perhaps it will reveal similarity between adjective\n",
        "#like adjective classes. but from what i read its more like hierachical phrases\n",
        "#nearest representation in glove doesnt work damnit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_NIL9m49oRR",
        "colab_type": "text"
      },
      "source": [
        "#Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OljmekheMNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import IntEnum\n",
        "from collections import defaultdict\n",
        "class wimpt():\n",
        "    STRONG_DISCARD = 1\n",
        "    WEAK_DISCARD = 0.5\n",
        "    WEAK_KEEP = 0.1\n",
        "    STRONG_KEEP = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TpJ-2oHeo6E",
        "colab_type": "code",
        "outputId": "c2f6eef7-e9ea-4b7f-bb63-7f70388414c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#Root is usually discarded, but is the starting point of calculation anyways\n",
        "#checks if any one of specified strings in dep_ then moves on\n",
        "#RULE TREE FORMAT:\n",
        "#Nodes are list of strings checking any(str in dep), 2nd element is either node or leaf\n",
        "#wimpt.X are leaves of tree\n",
        "\n",
        "\n",
        "\n",
        "parse_format = (\n",
        "    (('subj','agent'),wimpt.STRONG_KEEP),\n",
        "    (('acl'),())\n",
        ")\n",
        "def explore(sent):\n",
        "    impts = defaultdict(int)\n",
        "    impts[sent.root.i] += wimpt.STRONG_DISCARD\n",
        "        for child in sent.root.children:\n",
        "            if 'subj' in child.dep_:\n",
        "                impts[child.i] += wimpt.STRONG_KEEP\n",
        "            elif 'obj' in child.dep_:\n",
        "                impts[child.i] += wimpt.STRONG_KEEP\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-8a52df0048ea>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    for child in sent.root.children:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqk6N_DQZtQy",
        "colab_type": "code",
        "outputId": "06a64d6d-b88c-43ea-86b3-748117657d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "doc = nlp('The government is not important')\n",
        "displacy.render(doc, style=\"dep\",jupyter=True,options=displacy_options)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b1f499a8c6404040811983cb441ed9b3-0\" class=\"displacy\" width=\"370\" height=\"142.0\" direction=\"ltr\" style=\"max-width: none; height: 142.0px; color: #FF00FF; background: #00FF00; font-family: Comic Sans MS; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The government</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">not</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"102.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">important</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1f499a8c6404040811983cb441ed9b3-0-0\" stroke-width=\"2px\" d=\"M62,82.0 62,68.66666666666667 127.0,68.66666666666667 127.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1f499a8c6404040811983cb441ed9b3-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M62,84.0 L58,76.0 66,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1f499a8c6404040811983cb441ed9b3-0-1\" stroke-width=\"2px\" d=\"M142,82.0 142,68.66666666666667 207.0,68.66666666666667 207.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1f499a8c6404040811983cb441ed9b3-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M207.0,84.0 L211.0,76.0 203.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1f499a8c6404040811983cb441ed9b3-0-2\" stroke-width=\"2px\" d=\"M142,82.0 142,55.33333333333333 290.0,55.33333333333333 290.0,82.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1f499a8c6404040811983cb441ed9b3-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M290.0,84.0 L294.0,76.0 286.0,76.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wEi9o-ZS0MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#Referential strategy? Get Named Entity most frequent in passage, append to PRON sentences.\n",
        "def processToken(t):\n",
        "    if t in ignored: return None\n",
        "    elif 'subj' in t.dep_: processSubject(t)\n",
        "\n",
        "def processSubject(t):\n",
        "    if 'pass' in t.dep_:\n",
        "        pass\n",
        "    else:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_k8hUvqcbPy",
        "colab_type": "text"
      },
      "source": [
        "Unable to distinguish significant nouns? Perhaps case of insignificant nouns too little to matter.\n",
        "\n",
        "Always throw away pronouns till coreference is solved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdYA5nIDZ39W",
        "colab_type": "text"
      },
      "source": [
        "- of any active subject\n",
        "    - NOUN\n",
        "        - relcl: throw away\n",
        "        - acl: keep only if agent -> pobj is PROPN\n",
        "    - PRON: always discard (referential)\n",
        "    - PROPN: always keep it & related words\n",
        "        - unless copular verb (when not governing object), strongly discard verb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tgr5_MENHM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list((ori.text.strip(),query) for ori,query in doc._.queries if query != '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnYmGcHP_2sP",
        "colab_type": "text"
      },
      "source": [
        "Excluded tokens:\n",
        "- WRB: 5W1Hs are apparently ADVs?\n",
        "- All pronouns\n",
        "- WDT: unlikely to see 'what object?' type of sentences\n",
        "- UH: OBJECTION!\n",
        "- TO: apparently not impt\n",
        "- SYM: symbols\n",
        "- SP: space\n",
        "- RP\n",
        "- ... gives up listing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UWWrRLPFpZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://spacy.io/usage/rule-based-matching#adding-patterns-attributes\n",
        "ruler = EntityRuler(nlp)\n",
        "patterns = [\n",
        "            {'label':'PERSONAL','pattern':[\n",
        "                {},\n",
        "                {}\n",
        "            ]},\n",
        "            {'label':'PERSONAL','pattern':[\n",
        "                {},\n",
        "                {}\n",
        "            ]}\n",
        "            ]\n",
        "ruler.add_patterns(patterns)\n",
        "nlp.add_pipe(ruler,before='ner')\n",
        "# ^makes NER slightly smarter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5W0B8e_92Hr",
        "colab_type": "text"
      },
      "source": [
        "# Spacy tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7pGKFoDmEWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import wmd\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.add_pipe(wmd.WMD.SpacySimilarityHook(nlp), last=True)\n",
        "doc1 = nlp(\"Politician speaks to the media in Illinois.\")\n",
        "doc2 = nlp(\"The president greets the press in Chicago.\")\n",
        "print(doc1.similarity(doc2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL5b21ofnsAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#better lemmatization at a performance cost\n",
        "import spacy\n",
        "import lemminflect\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp('I am testing this example.')\n",
        "doc[2]._.lemma()         # 'test'\n",
        "doc[4]._.inflect('NNS')  # 'examples'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIHVNTuG98kq",
        "colab_type": "text"
      },
      "source": [
        "# Bert Extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I90rDmBflBRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardX\n",
        "!pip install pyrouge\n",
        "!pip install pytorch-transformers\n",
        "!pip install torch==1.1.0\n",
        "\n",
        "chdir('/content/drive/My Drive/CSIT Internship/code/PreSumm/src')\n",
        "import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import BertModel\n",
        "\n",
        "from models.encoder import ExtTransformerEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReTBaj2MSxuz",
        "colab_type": "text"
      },
      "source": [
        "## Classses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc9Bk7hb_bOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pretty much taken from the args parameters defined in train.py\n",
        "class lolConfig():\n",
        "    ext_ff_size = 2048\n",
        "    ext_heads = 8\n",
        "    ext_hidden_size = 768\n",
        "    ext_layers = 2\n",
        "    ext_dropout = 0.2\n",
        "    large = False #whether to use 'bert-large-uncased' or 'bert-base-uncased' from pytorch-transformers\n",
        "    temp_dir = '../temp' #useless\n",
        "    finetune_bert = False #whether output of bert model is detached\n",
        "    max_pos = 512\n",
        "    param_init = 'nan' #shouldnt be used since we are loading from checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXPgAVzWwvo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#taken from nlpyang/PreSumm/src/models/model_builder.py; minimally modified for a more applicative purpose\n",
        "#actually seems as if model was trained on top of default BERT? cause theres only 1 checkpoint for both large & base\n",
        "\n",
        "#obviously a wrapper. actually useless though.\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, large, temp_dir, finetune=False):\n",
        "        super(Bert, self).__init__()\n",
        "        if(large):\n",
        "            self.model = BertModel.from_pretrained('bert-large-uncased', cache_dir=temp_dir)\n",
        "        else:\n",
        "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
        "\n",
        "        self.finetune = finetune\n",
        "\n",
        "    def forward(self, x, segs, mask):\n",
        "        if(self.finetune):\n",
        "            top_vec, _ = self.model(x, segs, attention_mask=mask)\n",
        "        else:\n",
        "            self.eval()\n",
        "            with torch.no_grad():\n",
        "                top_vec, _ = self.model(x, segs, attention_mask=mask)\n",
        "        return top_vec\n",
        "\n",
        "class ExtSummarizer(nn.Module):\n",
        "    def __init__(self, args, device, checkpoint):\n",
        "        super(ExtSummarizer, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.bert = Bert(args.large, args.temp_dir, args.finetune_bert)\n",
        "\n",
        "        if(args.max_pos>512):\n",
        "            my_pos_embeddings = nn.Embedding(args.max_pos, self.bert.model.config.hidden_size)\n",
        "            my_pos_embeddings.weight.data[:512] = self.bert.model.embeddings.position_embeddings.weight.data\n",
        "            my_pos_embeddings.weight.data[512:] = self.bert.model.embeddings.position_embeddings.weight.data[-1][None,:].repeat(args.max_pos-512,1)\n",
        "            self.bert.model.embeddings.position_embeddings = my_pos_embeddings\n",
        "\n",
        "        self.ext_layer = ExtTransformerEncoder(self.bert.model.config.hidden_size, args.ext_ff_size, args.ext_heads,\n",
        "                                               args.ext_dropout, args.ext_layers)\n",
        "\n",
        "        if checkpoint is not None:\n",
        "            self.load_state_dict(checkpoint['model'], strict=True)\n",
        "        '''\n",
        "        else:\n",
        "            if args.param_init != 0.0:\n",
        "                for p in self.ext_layer.parameters():\n",
        "                    p.data.uniform_(-args.param_init, args.param_init)\n",
        "            if args.param_init_glorot:\n",
        "                for p in self.ext_layer.parameters():\n",
        "                    if p.dim() > 1:\n",
        "                        xavier_uniform_(p)\n",
        "        '''\n",
        "        self.to(device)\n",
        "\n",
        "    '''\n",
        "    Lol some docscript practice time\n",
        "    @param src one-hot tokens (padded)\n",
        "    @param segs alternating type ids of 1 & 0 demarcating sentences (padded)\n",
        "    @param clss indices of [CLS] tokens\n",
        "    @param mask_src attention mask for src (ie 1 for everything, 0 for pads)\n",
        "    @param mask_cls stateful mask of [CLS] tokens for training. Setting as all 1.\n",
        "    '''\n",
        "    #src = idxs, segs = alternating 1 & 0 type ids, clss = indices of [CLS] token\n",
        "    def forward(self, src, segs, clss, mask_src, mask_cls):\n",
        "        top_vec = self.bert(src, segs, mask_src)\n",
        "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
        "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
        "        return sent_scores, mask_cls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIJBjzwBy64n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "models.__path__.append('/content/drive/My Drive/CSIT Internship/code/PreSumm/src/models') #workaround for importing\n",
        "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers', 'encoder', 'ff_actv', 'use_interval', 'rnn_size']\n",
        "class sentence_scorer():\n",
        "\n",
        "    def load_model(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        checkpoint = torch.load('../models/bertext_cnndm_transformer.pt', map_location=lambda storage, loc: storage)\n",
        "        opt = vars(checkpoint['opt'])\n",
        "        for k in opt.keys():\n",
        "            if k in model_flags:\n",
        "                setattr(lolConfig, k, opt[k])\n",
        "        self.model = ExtSummarizer(lolConfig(), 'cuda', checkpoint)\n",
        "\n",
        "    def __call__(self,doc):\n",
        "        src = []\n",
        "        segs = []\n",
        "        clss = []\n",
        "        isZero = True\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            enc = self.tokenizer.encode_plus(sent.text)\n",
        "            if len(src) + len(enc['input_ids']) > lolConfig.max_pos: continue\n",
        "\n",
        "            clss.append(len(src))\n",
        "            src += enc['input_ids']\n",
        "\n",
        "            if isZero: segs += [0] * len(enc['input_ids'])\n",
        "            else: segs += [1] * len(enc['input_ids'])\n",
        "            isZero = not isZero\n",
        "\n",
        "        mask_cls = [1] * len(clss)\n",
        "\n",
        "        inilen = len(src)\n",
        "        src += [0]*(lolConfig.max_pos - inilen)\n",
        "        segs += [0]*(lolConfig.max_pos - inilen)\n",
        "        mask_src = [1]*inilen + [0]*(lolConfig.max_pos - inilen)\n",
        "        \n",
        "        src = torch.tensor([src],device='cuda')\n",
        "        segs = torch.tensor([segs],device='cuda')\n",
        "        clss = torch.tensor([clss],device='cuda')\n",
        "        mask_src = torch.tensor([mask_src],device='cuda',dtype=torch.uint8)\n",
        "        mask_cls = torch.tensor([mask_cls],device='cuda',dtype=torch.uint8)\n",
        "\n",
        "        result = sorted(zip(self.model(src,segs,clss,mask_src,mask_cls)[0][0].detach().cpu().numpy(),doc.sents),reverse=True)\n",
        "        \n",
        "        doc.set_extension('sentence_scores',default=result,force=True)\n",
        "        return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpc9nw7Eyn1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#how to deal with token limit? (dont for now i guess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tg1nDnYNmlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "suffering = sentence_scorer()\n",
        "suffering.load_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qJkrA4OXdb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "cc676d27-7fcb-4e65-9aaa-8644b7ce6bbe"
      },
      "source": [
        "doc2 = nlp(alpha2digit(test2,'en'))\n",
        "suffering(doc2)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.40555984,\n",
              "  More than 40 years after buying Eng's, a Chinese-American restaurant in the Hudson Valley, Tom Sit is reluctantly considering retirement.  ),\n",
              " (0.25322026,\n",
              "  Across the country, owners of Chinese-American restaurants like Eng's are ready to retire but have no one to pass the business to.),\n",
              " (0.18713969,\n",
              "  According to new data from the restaurant reviewing website Yelp, the share of Chinese restaurants in the top 20 metropolitan areas has been consistently falling.),\n",
              " (0.12939025,\n",
              "  5 years ago, an average of 7.3 per cent of all restaurants in these areas were Chinese, compared with 6.5 per cent today.),\n",
              " (0.10123267,\n",
              "  On Yelp, the average share of page views of Chinese restaurants hasn't declined, nor has the average rating.  ),\n",
              " (0.07528544,\n",
              "  That reflects 1,200 fewer Chinese restaurants at a time when these 20 places added more than 15,000 restaurants overall.  ),\n",
              " (0.070201315,\n",
              "  Even in San Francisco, home to the oldest Chinatown in the United States, the share of Chinese restaurants shrank to 8.8 per cent from 10 per cent.  ),\n",
              " (0.0667754,\n",
              "  For much of his life, Mr Sit has worked here 7 days a week, 12 hours a day.),\n",
              " (0.06037843,\n",
              "  Their children, educated and raised in America, are pursuing professional careers that do not demand the same gruelling labor as food service.  ),\n",
              " (0.06027483,\n",
              "  But his grown daughters, who have college degrees and good-paying jobs, don't intend to take over.  ),\n",
              " (0.053541142,\n",
              "  The restaurant business has always been tough, and rising rents and delivery apps haven't helped.),\n",
              " (0.051210206,\n",
              "  He cooks in the same kitchen where he worked as a young immigrant from China.),\n",
              " (0.045505334, Working 80 hours a week is just too hard.  ),\n",
              " (0.04028381,\n",
              "  And at the same time, the percentages of Indian, Korean and Vietnamese restaurants - many of which were also owned and operated by immigrants from Asian countries - are holding steady or increasing nationwide.  ),\n",
              " (0.03905272,\n",
              "  2 years ago, at the insistence of his wife, Mrs Faye Lee Sit, he started taking off one day a week.),\n",
              " (0.038846698, It doesn't seem that interest in the cuisine has faltered.),\n",
              " (0.029412266,\n",
              "  Tightening regulations on immigration and accounting have also made it harder for cash-based restaurants to do business.  ),\n",
              " (0.02732002, He's 76, and they're going to be grandparents soon.),\n",
              " (0.024532368,\n",
              "  But those are not factors specific to Chinese restaurants, and do not explain the wave of closings.),\n",
              " (0.024008038,\n",
              "  Instead, a big reason seems to be the economic mobility of the second generation.  ),\n",
              " (0.022731772, Still, it's not sustainable.),\n",
              " (0.016464416,\n",
              "  He seats his regulars at the same tables where his 3 daughters did homework.  ),\n",
              " (0.012454408,\n",
              "  He parks in the same space where he'd take breaks and read his wife's letters, sent from Montreal while they courted by post in the late 1970s.)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzkRHexzOtj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2 = '''\n",
        "More than 40 years after buying Eng's, a Chinese-American restaurant in the Hudson Valley, Tom Sit is reluctantly considering retirement.\n",
        "\n",
        "For much of his life, Mr Sit has worked here seven days a week, 12 hours a day. He cooks in the same kitchen where he worked as a young immigrant from China. He parks in the same space where he'd take breaks and read his wife's letters, sent from Montreal while they courted by post in the late 1970s. He seats his regulars at the same tables where his three daughters did homework.\n",
        "\n",
        "Two years ago, at the insistence of his wife, Mrs Faye Lee Sit, he started taking off one day a week. Still, it's not sustainable. He's 76, and they're going to be grandparents soon. Working 80 hours a week is just too hard.\n",
        "\n",
        "But his grown daughters, who have college degrees and good-paying jobs, don't intend to take over.\n",
        "\n",
        "Across the country, owners of Chinese-American restaurants like Eng's are ready to retire but have no one to pass the business to. Their children, educated and raised in America, are pursuing professional careers that do not demand the same gruelling labor as food service.\n",
        "\n",
        "According to new data from the restaurant reviewing website Yelp, the share of Chinese restaurants in the top 20 metropolitan areas has been consistently falling. Five years ago, an average of 7.3 per cent of all restaurants in these areas were Chinese, compared with 6.5 per cent today. That reflects 1,200 fewer Chinese restaurants at a time when these 20 places added more than 15,000 restaurants overall.\n",
        "\n",
        "Even in San Francisco, home to the oldest Chinatown in the United States, the share of Chinese restaurants shrank to 8.8 per cent from 10 per cent.\n",
        "\n",
        "It doesn't seem that interest in the cuisine has faltered. On Yelp, the average share of page views of Chinese restaurants hasn't declined, nor has the average rating.\n",
        "\n",
        "And at the same time, the percentages of Indian, Korean and Vietnamese restaurants - many of which were also owned and operated by immigrants from Asian countries - are holding steady or increasing nationwide.\n",
        "\n",
        "The restaurant business has always been tough, and rising rents and delivery apps haven't helped. Tightening regulations on immigration and accounting have also made it harder for cash-based restaurants to do business.\n",
        "\n",
        "But those are not factors specific to Chinese restaurants, and do not explain the wave of closings. Instead, a big reason seems to be the economic mobility of the second generation.\n",
        "\n",
        "\"It's a success that these restaurants are closing,\" said Ms Jennifer Lee, a former New York Times journalist who wrote of the rise of Chinese restaurants in her book The Fortune Cookie Chronicles and produced a documentary The Search For General Tso.\n",
        "\n",
        "\"These people came to cook so their children wouldn't have to, and now their children don't have to.\"\n",
        "\n",
        "The retirements of the restaurant owners also reflect the history of Chinese immigration to the United States. In 1882, the Chinese Exclusion Act halted what had been a steady rise in people coming from China. It was not revoked until 1943, and large-scale immigration resumed only after 1965, when other race-targeting quotas were abolished.\n",
        "\n",
        "China's Cultural Revolution, an often-violent social and political upheaval that started in 1966, prompted many young people to emigrate to the US, a country that projected an image of freedom and economic possibility.\n",
        "\n",
        "Mr Sit left Guangzhou, in southern China, in 1968. He hiked, climbed and swam his way to Hong Kong, filling his pants with pine cones as an improvised flotation device.\n",
        "\n",
        "\"There was just no future,\" he said. \"The only way to get freedom and to get a good job was to go to Hong Kong.\"\n",
        "\n",
        "In 1974, he immigrated to the US and started working at Eng's, which opened in 1927. He had never worked in a restaurant before, but the heat from the woks was much less intense than what he experienced at a Hong Kong plastics factory where he had worked.\n",
        "\n",
        "Unlike Mr Sit, some immigrants had been chefs in China. They served Hunan and Cantonese foods on linen tablecloths to bejewelled, curious diners at places like Shun Lee Palace in New York.\n",
        "\n",
        "\"There was the golden age of Chinese cooking in America, starting in the late 1960s and early 1970s,\" said Mr Ed Schoenfeld, a restaurateur and chef who had worked in Chinese restaurants since the 1970s. \"We started getting regional practitioners of fine regional cuisine to come to this country and do their thing.\"\n",
        "\n",
        "Mostly, though, the newly minted chefs cooked quickly and cheaply. They adapted their method of cooking to American tastes, developing dishes like beef chow fun, fortune cookies and egg drop soup, often brought home in the signature takeout containers.\n",
        "\n",
        "\"They were not precious,\" Ms Lee said. \"These people did not come to be chefs; they came to be immigrants, and cooking was the way they made a living.\"\n",
        "\n",
        "Other immigrant groups follow a similar pattern. With social mobility and inclusion in more mainstream parts of the economy, the children of immigrants are less likely than their parents to own their own businesses.\n",
        "\n",
        "\"In some ways, the children are regaining the status of the first generation that they have lost while migrating,\" said sociology professor Jennifer Lee of Columbia University and co-author of The Asian American Achievement Paradox. (She is not related to Ms Lee, the journalist.) \"The goal has never been to continue those businesses.\"\n",
        "\n",
        "When they do become entrepreneurs, these children tend to work in industries like tech or consulting, rather than in food service or nail salons.\n",
        "\n",
        "In the past decade, some members of the second generation have also chosen to take charge of family restaurants. Nom Wah Tea Parlor, a New York dim sum restaurant that opened in 1920, has stayed a family business: first run by the Choy family, then the Tangs.\n",
        "\n",
        "The 41-year-old owner, Mr Wilson Tang, left a career in finance to succeed his uncle in 2011. Initially, his parents baulked at his decision.\n",
        "\n",
        "\"As immigrants, it's the only thing you can do; if it's not restaurants, it's a laundromat,\" he said. \"For me to choose to go back to owning a restaurant? That was tough for them to accept.\"\n",
        "\n",
        "Since then, Nom Wah has expanded - to another Manhattan location, to Philadelphia and to Shenzhen, China. On any given night, groups of guests wait for a table outside the Chinatown location for up to an hour, huddled in the bend of Doyers Street.\n",
        "\n",
        "\"I had this unique opportunity to preserve something that was from old New York,\" Mr Tang said. \"I still work extremely hard. But I also know how to use marketing tools, like the Internet.\"\n",
        "\n",
        "In a parallel effort, the team behind Junzi Kitchen, a fast-casual Chinese restaurant chain based in New York, recently raised US$5 million (S$6.7 million) to research and buy places like Eng's, rebranding them with Junzi's modern take on the cuisine.\n",
        "\n",
        "\"They are still going to have their usual beloved Chinese takeout services, but we are providing an upgraded version of that,\" said Mr Yong Zhao, the founder and chief executive.\n",
        "\n",
        "But family-run Chinese restaurants are typically not being passed to the next generation. Some may close up shop, sell their businesses to other first-generation immigrants or move on and see their former storefronts become something else entirely.\n",
        "\n",
        "Mr Sit has not yet found the right person to run the restaurant, and has no immediate plans to close.\n",
        "\n",
        "\"To take over Eng's, you have to keep the heart in Eng's,\" he said. \"You need to have a loyalty to the business, not just someone who thinks, 'I'll make one year, two years of money, I don't care.'\"\n",
        "\n",
        "Mrs Sit feels more ready to retire than her husband. Normally talkative, he can be evasive whenever the family tries to bring up a successor.\n",
        "\n",
        "\"They'll have to work hard,\" she said, her eyes sparkling as she teased her husband. \"Like Tom Sit. Maybe then he'll let them take over.\"\n",
        "\n",
        "If he ever actually does hand Eng's to someone else, Mr Sit will miss his customers, and miss running an operation.\n",
        "\n",
        "But he is proud of what he built. He is proud that his daughters, American-born educated professionals, are working in jobs they have chosen, jobs they love.\n",
        "\n",
        "\"I hoped they have a better life than me,\" he said. \"A good life. And they do.\"\n",
        "'''\n",
        "test2 = ' '.join(test2.splitlines())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}